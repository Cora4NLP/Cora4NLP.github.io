[{"authors":["cristina-espana"],"categories":null,"content":"","date":1677110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677110400,"objectID":"03b0e63928d4828562e76d3d9421b170","permalink":"https://cora4nlp.github.io/author/cristina-espana-i-bonet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cristina-espana-i-bonet/","section":"authors","summary":"","tags":["Researchers"],"title":"Cristina España i Bonet","type":"authors"},{"authors":["jingyi-zhang"],"categories":null,"content":"","date":1677110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677110400,"objectID":"33a41c6adcffdbccfd2a15d0f3501993","permalink":"https://cora4nlp.github.io/author/jingyi-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingyi-zhang/","section":"authors","summary":"","tags":["Researchers"],"title":"Jingyi Zhang","type":"authors"},{"authors":["josef-van-genabith"],"categories":null,"content":"","date":1677110400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677110400,"objectID":"d654f4176667d586adc2001840628351","permalink":"https://cora4nlp.github.io/author/josef-van-genabith/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/josef-van-genabith/","section":"authors","summary":"","tags":["Principal Investigators"],"title":"Josef van Genabith","type":"authors"},{"authors":["david-harbecke"],"categories":null,"content":"","date":1666427583,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1666427583,"objectID":"57423a933a38af75c718059324719b6e","permalink":"https://cora4nlp.github.io/author/david-harbecke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-harbecke/","section":"authors","summary":"","tags":["PhD Students"],"title":"David Harbecke","type":"authors"},{"authors":["leonhard-hennig"],"categories":null,"content":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available. As a project lead, I\u0026rsquo;ve managed various national research projects, such as Smart Data Web, PLASS, DAYSTREAM, and the DFKI part in the Berlin Big Data Center, as well as industry-funded projects, e.g. for Deutsche Telekom and Lenovo.\n","date":1666427583,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1666427583,"objectID":"76acb2a1dbfa728042427546fca4cab6","permalink":"https://cora4nlp.github.io/author/leonhard-hennig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leonhard-hennig/","section":"authors","summary":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available.","tags":["Researchers"],"title":"Leonhard Hennig","type":"authors"},{"authors":["arne-binder"],"categories":null,"content":"","date":1666310400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1666310400,"objectID":"db472dfe584699b02c3fb0b542f4efba","permalink":"https://cora4nlp.github.io/author/arne-binder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/arne-binder/","section":"authors","summary":"","tags":["PhD Students"],"title":"Arne Binder","type":"authors"},{"authors":["nils-rethmeier"],"categories":null,"content":"","date":1665736383,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665736383,"objectID":"5fa9f4aadfb6bfa3e5e6ad75c3611835","permalink":"https://cora4nlp.github.io/author/nils-rethmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nils-rethmeier/","section":"authors","summary":"","tags":["PhD Students"],"title":"Nils Rethmeier","type":"authors"},{"authors":null,"categories":null,"content":"","date":1665532800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665532800,"objectID":"ac12c3e849a87511f0b8932e2a700083","permalink":"https://cora4nlp.github.io/author/saadullah-amin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/saadullah-amin/","section":"authors","summary":"","tags":["PhD Students"],"title":"Saadullah Amin","type":"authors"},{"authors":null,"categories":null,"content":"Christoph is a postdoctoral researcher at Humboldt University of Berlin and part of the Cluster Science of Intelligence. He received his Bachelor’s degree in Electrical Engineering from the University of Applied Sciences Munich and his Master’s degree in Computer Engineering from the Technical University of Berlin. For his doctoral degree in Computer Science at Technical University of Berlin and the German Research Center for AI (DFKI), he studied the generalizability and data efficiency of neural-network-based natural language processing (NLP) approaches, in particular, how to learn from limited amounts of labeled data and transfer linguistic knowledge between related NLP tasks.\n","date":1653523200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653523200,"objectID":"55f7fc0a0becc469231bd11edf9d90c1","permalink":"https://cora4nlp.github.io/author/christoph-alt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christoph-alt/","section":"authors","summary":"Christoph is a postdoctoral researcher at Humboldt University of Berlin and part of the Cluster Science of Intelligence. He received his Bachelor’s degree in Electrical Engineering from the University of Applied Sciences Munich and his Master’s degree in Computer Engineering from the Technical University of Berlin.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["tatiana-anikina"],"categories":null,"content":"","date":1637193600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1637193600,"objectID":"ec7d5b5ff62925a22fe4525ccbc2a55d","permalink":"https://cora4nlp.github.io/author/tatiana-anikina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tatiana-anikina/","section":"authors","summary":"","tags":["PhD Students"],"title":"Tatiana Anikina","type":"authors"},{"authors":null,"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617235200,"objectID":"05deb29b20577d1ba94a0381a241684f","permalink":"https://cora4nlp.github.io/author/stalin-varanasi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/stalin-varanasi/","section":"authors","summary":"","tags":["PhD Students"],"title":"Stalin Varanasi","type":"authors"},{"authors":["guenter-neumann"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"927589e6c78af66aca57466cc9bf166b","permalink":"https://cora4nlp.github.io/author/guenter-neumann/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guenter-neumann/","section":"authors","summary":"","tags":["Researchers"],"title":"Guenter Neumann","type":"authors"},{"authors":["sebastian-moeller"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"881850e20e8b4f43d7ab901d58ccc663","permalink":"https://cora4nlp.github.io/author/sebastian-moller/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sebastian-moller/","section":"authors","summary":"","tags":["Principal Investigators"],"title":"Sebastian Möller","type":"authors"},{"authors":[],"categories":[],"content":"We are happy to announce that two papers from Cora4NLP members have been accepted for publication at the 17th Conference of the European Chapter of the Association for Computational Linguistics. The conference will take place from May2nd to May 6th, 2023.\n  Sonal Sannigrahi, Cristina España i Bonet, Josef van Genabith  (2023). Are the Best Multilingual Document Embeddings Simply Based on Sentence Embeddings?. EACL 2023.  Cite    Yusser Al Ghussin, Jingyi Zhang, Josef van Genabith  (2023). Exploring Paracrawl for Document-level Neural Machine Translation. EACL 2023.  Cite   ","date":1677133441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677133441,"objectID":"bb523e94190d86ae58d6177d523fcceb","permalink":"https://cora4nlp.github.io/post/eacl2023/","publishdate":"2023-02-23T08:24:01+02:00","relpermalink":"/post/eacl2023/","section":"post","summary":"We are happy to announce that two papers from Cora4NLP members have been accepted for publication at the 17th Conference of the European Chapter of the Association for Computational Linguistics. The conference will take place from May2nd to May 6th, 2023.","tags":[],"title":"2 papers accepted at EACL 2023","type":"post"},{"authors":["Sonal Sannigrahi","Cristina España i Bonet","Josef van Genabith"],"categories":[],"content":"","date":1677110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677110400,"objectID":"690f202188398b803b33bce4ed4de747","permalink":"https://cora4nlp.github.io/publication/sannigrahi-etal-eacl-2023-multilingual/","publishdate":"2023-02-23T00:00:00Z","relpermalink":"/publication/sannigrahi-etal-eacl-2023-multilingual/","section":"publication","summary":"","tags":[],"title":"Are the Best Multilingual Document Embeddings Simply Based on Sentence Embeddings?","type":"publication"},{"authors":["Yusser Al Ghussin","Jingyi Zhang","Josef van Genabith"],"categories":[],"content":"","date":1677110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677110400,"objectID":"71b60903c814c71352d44857ac5e0c1e","permalink":"https://cora4nlp.github.io/publication/ghussin-etal-eacl-2023-exploring/","publishdate":"2023-02-23T00:00:00Z","relpermalink":"/publication/ghussin-etal-eacl-2023-exploring/","section":"publication","summary":"","tags":[],"title":"Exploring Paracrawl for Document-level Neural Machine Translation","type":"publication"},{"authors":["Niyati Bafna","Josef van Genabith","Cristina España i Bonet","Zdeněk Žabokrtský"],"categories":[],"content":"","date":1670401983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670401983,"objectID":"fbef726463af196d4c862f5478217421","permalink":"https://cora4nlp.github.io/publication/conll2022-bafna-combining/","publishdate":"2022-12-07T10:33:03+02:00","relpermalink":"/publication/conll2022-bafna-combining/","section":"publication","summary":"We present a novel method for unsupervised cognate/borrowing identification from monolingual corpora designed for low and extremely low resource scenarios, based on combining noisy semantic signals from joint bilingual spaces with orthographic cues modelling sound change. We apply our method to the North Indian dialect continuum, containing several dozens of dialects and languages spoken by more than 100 million people. Many of these languages are zero-resource and therefore natural language processing for them is non-existent. We first collect monolingual data for 26 Indic languages, 16 of which were previously zero-resource, and perform exploratory character, lexical and subword cross-lingual alignment experiments for the first time at this scale on this dialect continuum. We create bilingual evaluation lexicons against Hindi for 20 of the languages. We then apply our cognate identification method on the data, and show that our method outperforms both traditional orthography baselines as well as EM-style learnt edit distance matrices. To the best of our knowledge, this is the first work to combine traditional orthographic cues with noisy bilingual embeddings to tackle unsupervised cognate detection in a (truly) low-resource setup, showing that even noisy bilingual embeddings can act as good guides for this task. We release our multilingual dialect corpus, called HinDialect, as well as our scripts for evaluation data collection and cognate induction.","tags":[],"title":"Combining Noisy Semantic Signals with Orthographic Cues: Cognate Induction for the Indic Dialect Continuum","type":"publication"},{"authors":[],"categories":[],"content":"One paper from Cora4NLP authors has been accepted for publication at CoNLL 2022, the SIGNLL Conference on Computational Natural Language Learning. The conference is co-located with EMNLP 2022 and planned to be a hybrid meeting. It will take place in Abu Dhabi, from Dec 7th to Dec 8th, 2022. The paper introduces HinDialect, a new corpus of 26 Hindi dialects, and presents a new method for cognate induction with low-resource languages.\n Niyati Bafna, Josef van Genabith, Cristina España i Bonet, Zdeněk Žabokrtský  (2022). Combining Noisy Semantic Signals with Orthographic Cues: Cognate Induction for the Indic Dialect Continuum. CoNLL 2022.  PDF  Cite   ","date":1670311441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670311441,"objectID":"4cb72f55daefb04f0a5995f4348d2849","permalink":"https://cora4nlp.github.io/post/conll2022/","publishdate":"2022-12-06T09:24:01+02:00","relpermalink":"/post/conll2022/","section":"post","summary":"One paper from Cora4NLP authors has been accepted for publication at CoNLL 2022, the SIGNLL Conference on Computational Natural Language Learning. The conference is co-located with EMNLP 2022 and planned to be a hybrid meeting.","tags":[],"title":"1 paper to be presented at CoNLL 2022","type":"post"},{"authors":["Yuxuan Chen","David Harbecke","Leonhard Hennig"],"categories":[],"content":"","date":1666427583,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666427583,"objectID":"4c041d8a26606010a045082e8f180e45","permalink":"https://cora4nlp.github.io/publication/emnlp2022-chen-meffiprompt/","publishdate":"2022-10-22T10:33:03+02:00","relpermalink":"/publication/emnlp2022-chen-meffiprompt/","section":"publication","summary":"Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the high cost of handcrafting multilingual prompts. In this paper, we present the first work on prompt-based multilingual relation classification (RC), by introducing an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels. We evaluate its performance in fully supervised, few-shot and zero-shot scenarios, and analyze its effectiveness across 14 languages, prompt variants, and English-task training in cross-lingual settings. We find that in both fully supervised and few-shot scenarios, our prompt method beats competitive baselines: fine-tuning XLM-R-EM and null prompts. It also outperforms the random baseline by a large margin in zero-shot experiments. Our method requires little in-language knowledge and can be used as a strong baseline for similar multilingual classification tasks.","tags":[],"title":"Multilingual Relation Classification via Efficient and Effective Prompting","type":"publication"},{"authors":["Arne Binder","Bhuvanesh Verma","Leonhard Hennig"],"categories":[],"content":"","date":1666310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666310400,"objectID":"4bb3720e75628da71f90ed6d4ddf47f7","permalink":"https://cora4nlp.github.io/publication/wiesp2022-binder-etal-full/","publishdate":"2022-10-21T00:00:00Z","relpermalink":"/publication/wiesp2022-binder-etal-full/","section":"publication","summary":"Scholarly Argumentation Mining (SAM) has recently gained attention due to its potential to help scholars with the rapid growth of published scientific literature. It comprises two subtasks: argumentative discourse unit recognition (ADUR) and argumentative relation extraction (ARE), both of which are challenging since they require e.g. the integration of domain knowledge, the detection of implicit statements, and the disambiguation of argument structure. While previous work focused on dataset construction and baseline methods for specific document sections, such as abstract or results, full-text scholarly argumentation mining has seen little progress. In this work, we introduce a sequential pipeline model combining ADUR and ARE for full-text SAM, and provide a first analysis of the performance of pretrained language models (PLMs) on both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus, outperforming the previous best reported result by a large margin (+7% F1). We also present the first results for ARE, and thus for the full AM pipeline, on this benchmark dataset. Our detailed error analysis reveals that non-contiguous ADUs as well as the interpretation of discourse connectors pose major challenges and that data annotation needs to be more consistent.","tags":[],"title":"Full-Text Argumentation Mining on Scientific Publications","type":"publication"},{"authors":["Malte Ostendorff","Nils Rethmeier","Isabelle Augenstein","Bela Gipp","Georg Rehm"],"categories":[],"content":"","date":1665736383,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665736383,"objectID":"db6ae7985024e062c2e16cc599a3472b","permalink":"https://cora4nlp.github.io/publication/emnlp2022-ostendorff/","publishdate":"2022-10-14T10:33:03+02:00","relpermalink":"/publication/emnlp2022-ostendorff/","section":"publication","summary":"Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However, discrete citations enforce a hard cut-off to similarity. This is counter-intuitive to similarity-based learning, and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead, we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity, to sample hard-to-learn negatives and positives, and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-the-art on the SciDocs benchmark. Furthermore, we demonstrate that it can train (or tune) models sample-efficiently, and that it can be combined with recent training-efficient methods. Perhaps surprisingly, even training a general-domain language model this way outperforms baselines pretrained in-domain. ","tags":[],"title":"Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings","type":"publication"},{"authors":[],"categories":[],"content":"Two papers from Cora4NLP authors have been accepted for publication at EMNLP 2022, the 2022 Conference on Empirical Methods in Natural Language Processing. The conference is planned to be a hybrid meeting and will take place in Abu Dhabi, from Dec 7th to Dec 11th, 2022. The first paper introduces an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels, in the case of in-language prompting. We evaluate its performance in fully supervised, few-shot and zero-shot scenarios across 14 languages, soft prompt variants, and English-task training in cross-lingual settings. The second paper proposes neighborhood contrastive learning for the representation learning of scientific document and achieves new state-of-the-art results on the SciDocs benchmark.\n  Yuxuan Chen, David Harbecke, Leonhard Hennig  (2022). Multilingual Relation Classification via Efficient and Effective Prompting. EMNLP 2022.  PDF  Cite  Code  DOI    Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm  (2022). Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. EMNLP 2022.  PDF  Cite  Code  DOI   ","date":1665732241,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665732241,"objectID":"3db51a86704299a94615128c8e63eba2","permalink":"https://cora4nlp.github.io/post/emnlp2022/","publishdate":"2022-10-14T09:24:01+02:00","relpermalink":"/post/emnlp2022/","section":"post","summary":"Two papers from Cora4NLP authors have been accepted for publication at EMNLP 2022, the 2022 Conference on Empirical Methods in Natural Language Processing. The conference is planned to be a hybrid meeting and will take place in Abu Dhabi, from Dec 7th to Dec 11th, 2022.","tags":[],"title":"2 papers to be presented at EMNLP 2022","type":"post"},{"authors":[],"categories":[],"content":"One paper from Cora4NLP authors has been accepted for publication at the Workshop on Information Extraction from Scientific Publications (WIESP). The workshop will be held at AACL-IJCNLP 2022, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, which will take place as an online-only event from Nov. 20 to Nov. 23, 2022. The paper proposes a new method for argument mining in full-text scientific documents by combining argumentative discourse unit recognition with relation extraction.\n Arne Binder, Bhuvanesh Verma, Leonhard Hennig  (2022). Full-Text Argumentation Mining on Scientific Publications. WIESP 2022.  PDF  Cite  Code   ","date":1665728641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665728641,"objectID":"5ce70b7b52cd23896270f5442726b45e","permalink":"https://cora4nlp.github.io/post/aacl-ijcnlp2022/","publishdate":"2022-10-14T08:24:01+02:00","relpermalink":"/post/aacl-ijcnlp2022/","section":"post","summary":"One paper from Cora4NLP authors has been accepted for publication at the Workshop on Information Extraction from Scientific Publications (WIESP). The workshop will be held at AACL-IJCNLP 2022, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, which will take place as an online-only event from Nov.","tags":[],"title":"1 paper to be presented at AACL-IJCNLP 2022","type":"post"},{"authors":["Saadullah Amin","Pasquale Minervini","David Chang","Pontus Stenetorp","Günter Neumann"],"categories":[],"content":"","date":1665532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665532800,"objectID":"0f655561e5b1900a4f9f9afab81d81dd","permalink":"https://cora4nlp.github.io/publication/amin-coling2022-md19/","publishdate":"2022-08-21T00:00:00Z","relpermalink":"/publication/amin-coling2022-md19/","section":"publication","summary":"Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26% to 86%. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction.","tags":[],"title":"MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction","type":"publication"},{"authors":["Yuxuan Chen","Jonas Mikkelsen","Arne Binder","Christoph Alt","Leonhard Hennig"],"categories":[],"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"71450b988e01035d8dda07fd4d7aebf8","permalink":"https://cora4nlp.github.io/publication/acl2022-repl4nlp-chen-fewie/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/acl2022-repl4nlp-chen-fewie/","section":"publication","summary":"Pre-trained language models (PLM) are effective components of few-shot named entity recognition (NER) approaches when augmented with continued pre-training on task-specific out-of-domain data or fine-tuning on in-domain data. However, their performance in low-resource scenarios, where such data is not available, remains an open question. We introduce an encoder evaluation framework, and use it to systematically compare the performance of state-of-the-art pre-trained representations on the task of low-resource NER. We analyze a wide range of encoders pre-trained with different strategies, model architectures, intermediate-task fine-tuning, and contrastive learning. Our experimental results across ten benchmark NER datasets in English and German show that encoder performance varies significantly, suggesting that the choice of encoder for a specific low-resource scenario needs to be carefully evaluated.","tags":[],"title":"A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition","type":"publication"},{"authors":["David Harbecke","Yuxuan Chen","Leonhard Hennig","Christoph Alt"],"categories":[],"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"9cd7d48423cca6a139bf79bcc5d90221","permalink":"https://cora4nlp.github.io/publication/acl2022-nlppower-harbecke-f1/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/acl2022-nlppower-harbecke-f1/","section":"publication","summary":"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F1, macro-F1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.","tags":[],"title":"Why only Micro-$F_1$? Class Weighting of Measures for Relation Classification","type":"publication"},{"authors":["Saadullah Amin","Noon Pokaratsiri Goldstein","Morgan Wixted","Alejandro Garcia-Rudolph","Catalina Martinez-Costa","and Guenter Neumann"],"categories":[],"content":"","date":1652745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652745600,"objectID":"8fcfd5a16f62a8ebbeb09a66e687504f","permalink":"https://cora4nlp.github.io/publication/amin-acl-bionlp22-fewdeid/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/amin-acl-bionlp22-fewdeid/","section":"publication","summary":"Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information still lies in large volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected health information (PHI), are exposed to information extraction tools for downstream applications, risking patient identification. Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not suitable in real-world multilingual settings. Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings. In this work, we empirically show the few-shot cross-lingual transfer property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain. We annotate a gold evaluation dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training. Our model improves the zero-shot F1-score from 73.7% to 91.2% on the gold evaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019) from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual target corpus. When generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2%.","tags":[],"title":"Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts","type":"publication"},{"authors":["Ioannis Dikeoulias","Saadullah Amin","Günter Neumann"],"categories":[],"content":"","date":1652745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652745600,"objectID":"9b664eaa3d3c459f3eb0f40452fffb92","permalink":"https://cora4nlp.github.io/publication/dikeoulias-acl-repl4nlp-tlowfer/","publishdate":"2022-03-28T00:00:00Z","relpermalink":"/publication/dikeoulias-acl-repl4nlp-tlowfer/","section":"publication","summary":"Temporal knowledge graph completion (TKGC) has become a popular approach for reasoning over the event and temporal knowledge graphs, targeting the completion of knowledge with accurate but missing information. In this context, tensor decomposition has successfully modeled interactions between entities and relations. Their effectiveness in static knowledge graph completion motivates us to introduce Time-LowFER, a family of parameter-efficient and time-aware extensions of the low-rank tensor factorization model LowFER. Noting several limitations in current approaches to represent time, we propose a cycle-aware time-encoding scheme for time features, which is model-agnostic and offers a more generalized representation of time. We implement our methods in a unified temporal knowledge graph embedding framework, focusing on time-sensitive data processing. The experiments show that our proposed methods perform on par or better than the state-of-the-art semantic matching models on two benchmarks.","tags":[],"title":"Temporal Knowledge Graph Reasoning with Low-rank and Model-agnostic Representations","type":"publication"},{"authors":[],"categories":[],"content":"One paper from Cora4NLP researchers has been accepted for oral presentation at COLING 2022, the 29th International Conference on Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Gyeongju, Korea, from Oct 12th - Oct 17th, 2022. Saadullah Amin will present the paper \u0026ldquo;MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction\u0026rdquo;.\n Saadullah Amin, Pasquale Minervini, David Chang, Pontus Stenetorp, Günter Neumann  (2022). MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction. COLING 2022.  PDF  Cite  Code  Dataset   ","date":1648711441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648711441,"objectID":"1ee232c7b812f02d770925192088d59c","permalink":"https://cora4nlp.github.io/post/coling2022/","publishdate":"2022-03-31T09:24:01+02:00","relpermalink":"/post/coling2022/","section":"post","summary":"One paper from Cora4NLP researchers has been accepted for oral presentation at COLING 2022, the 29th International Conference on Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Gyeongju, Korea, from Oct 12th - Oct 17th, 2022.","tags":[],"title":"1 paper to be presented at COLING 2022","type":"post"},{"authors":[],"categories":[],"content":"Four papers from Cora4NLP researchers have been accepted for publication at ACL 2022, the 60th Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Dublin, Ireland, from May 22nd through May 27th, 2022. One paper is on evaluating pre-trained encoders on the task of low-resource NER across several English and German datasets, the other analyzes relation classification evaluation and suggests that using F1 weightings other than micro-F1 tells us much more about model performance, e.g. on imbalanced datasets. The third paper is about cross-lingual transfer learning for de-identification of clinical text, and the final paper addresses the task of temporal knowledge graph reasoning.\n  Yuxuan Chen, Jonas Mikkelsen, Arne Binder, Christoph Alt, Leonhard Hennig  (2022). A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition. ACL-REPL4NLP 2022.  PDF  Cite  Code    David Harbecke, Yuxuan Chen, Leonhard Hennig, Christoph Alt  (2022). Why only Micro-$F_1$? Class Weighting of Measures for Relation Classification. ACL-NLPPower 2022.  Cite    Saadullah Amin, Noon Pokaratsiri Goldstein, Morgan Wixted, Alejandro Garcia-Rudolph, Catalina Martinez-Costa, and Guenter Neumann  (2022). Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts. ACL-BioNLP 2022.  PDF  Cite    Ioannis Dikeoulias, Saadullah Amin, Günter Neumann  (2022). Temporal Knowledge Graph Reasoning with Low-rank and Model-agnostic Representations. ACL-RepL4NLP 2022.  PDF  Cite  Code   ","date":1648711441,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648711441,"objectID":"011bca29552463724be0a68ec689af6a","permalink":"https://cora4nlp.github.io/post/acl2022/","publishdate":"2022-03-31T09:24:01+02:00","relpermalink":"/post/acl2022/","section":"post","summary":"Four papers from Cora4NLP researchers have been accepted for publication at ACL 2022, the 60th Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Dublin, Ireland, from May 22nd through May 27th, 2022.","tags":[],"title":"4 papers to be presented at ACL 2022","type":"post"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"f9e8815b2031f0463636642979110763","permalink":"https://cora4nlp.github.io/publication/rethmeier-aaai-2022-longtail/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/rethmeier-aaai-2022-longtail/","section":"publication","summary":"For natural language processing 'text-to-text' tasks, the prevailing approaches heavily rely on pretraining large self-supervised models on increasingly larger 'task-external' data. Transfer learning from high-resource pretraining works well, but research has focused on settings with very large data and compute requirements, while the potential of efficient low-resource learning, without large 'task-external' pretraining, remains under-explored. In this work, we evaluate against three core challenges for resource efficient learning. Namely, we analyze: (1) pretraining data (X) efficiency; (2) zero to few-shot label (Y) efficiency; and (3) long-tail generalization, since long-tail preservation has been linked to algorithmic fairness and because data in the tail is limited by definition. To address these challenges, we propose a data and compute efficient self-supervised, contrastive text encoder, pretrained on 60MB of 'task-internal' text data, and compare it to RoBERTa, which was pretrained on 160GB of 'task-external' text. We find our method outperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa's fine-tuning time.","tags":[],"title":"Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data","type":"publication"},{"authors":["Natalia Skachkova","Cennet Oguz","Tatiana Anikina","Siyu Tao","Sharmila Upadhyaya","Ivana Kruijff-Korbayova"],"categories":[],"content":"","date":1637193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637193600,"objectID":"4d2b87cfceb9fb1933f05727642cb53c","permalink":"https://cora4nlp.github.io/publication/skachkova-emnlp-2021-codi/","publishdate":"2021-11-18T00:00:00Z","relpermalink":"/publication/skachkova-emnlp-2021-codi/","section":"publication","summary":"We compare our team's systems to others submitted for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. We analyse the architectures and performance, report some problematic cases in gold annotations, and suggest possible improvements of the systems, their evaluation, data annotation, and the organization of the shared task.","tags":[],"title":"Anaphora Resolution in Dialogue: Cross-Team Analysis of the DFKI-TalkingRobots Team Submissions for the CODI-CRAC 2021 Shared-Task","type":"publication"},{"authors":["Tatiana Anikina","Cennet Oguz","Natalia Skachkova","Siyu Tao","Sharmila Upadhyaya","Ivana Kruijff-Korbayova"],"categories":[],"content":"","date":1637193600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637193600,"objectID":"85160ba164205e9053e14b500997add2","permalink":"https://cora4nlp.github.io/publication/anikina-emnlp-2021-codi/","publishdate":"2021-11-18T00:00:00Z","relpermalink":"/publication/anikina-emnlp-2021-codi/","section":"publication","summary":"We describe the system developed by the DFKI-TalkingRobots Team for the CODI-CRAC 2021 Shared-Task on anaphora resolution in dialogue. Our system consists of three subsystems: (1) the Workspace Coreference System (WCS) incrementally clusters mentions using semantic similarity based on embeddings combined with lexical feature heuristics; (2) the Mention-to-Mention (M2M) coreference resolution system pairs same entity mentions; (3) the Discourse Deixis Resolution (DDR) system employs a Siamese Network to detect discourse anaphor-antecedent pairs. WCS achieved F1-score of 55.6% averaged across the evaluation test sets, M2M achieved 57.2% and DDR achieved 21.5%.","tags":[],"title":"Anaphora Resolution in Dialogue: Description of the DFKI-TalkingRobots System for the CODI-CRAC 2021 Shared-Task","type":"publication"},{"authors":["Jörg Steffen","Josef van Genabith"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"2497309a927e79fcde44af7daf28c75e","permalink":"https://cora4nlp.github.io/publication/steffen-emnlp-2021/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/publication/steffen-emnlp-2021/","section":"publication","summary":"For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but non-contiguous in target etc. Here we present TransIns, a system for non-plain text document translation that builds on the Okapi framework and MT models trained with Marian NMT. We develop, implement and evaluate different strategies for reinserting markup into translated sentences using token alignments between source and target sentences. We propose a simple and effective strategy that compiles down all markup to single source tokens and transfers them to aligned target tokens. A first evaluation shows that this strategy yields highly accurate markup in the translated documents that outperforms the markup quality found in documents translated with popular translation services. We release TransIns under the MIT License as open-source software on https://github.com/DFKI-MLT/TransIns. An online demonstrator is available at https://transins.dfki.de.","tags":[],"title":"TransIns: Document Translation with Markup Reinsertion","type":"publication"},{"authors":["Tatiana Anikina","Ivana Kruijff-Korbayova"],"categories":[],"content":"","date":1632960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632960000,"objectID":"99885bf12143e5e0d7400be3ac62468e","permalink":"https://cora4nlp.github.io/publication/anikina-semdial-2021-eveenti/","publishdate":"2021-09-30T00:00:00Z","relpermalink":"/publication/anikina-semdial-2021-eveenti/","section":"publication","summary":"We present the EveEnti (Event and Entity) annotation framework for events and entities in dialogue that we use to annotate several dialogues in German from the emergency response domain.","tags":[],"title":"Annotating events and entities in dialogue","type":"publication"},{"authors":["Leonhard Hennig","Phuc Tran Truong","Aleksandra Gabryszak"],"categories":[],"content":"","date":1630972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630972800,"objectID":"7b98766f43a9e140f9b34317ab45d6c9","permalink":"https://cora4nlp.github.io/publication/hennig-konvens-2021-mobie/","publishdate":"2021-08-11T00:00:00Z","relpermalink":"/publication/hennig-konvens-2021-mobie/","section":"publication","summary":"We present MobIE, a German-language dataset, which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,232 social media texts and traffic reports with 91K tokens, and contains 20.5K annotated entities, 13.1K of which are linked to a knowledge base. A subset of the dataset is human-annotated with seven mobility-related, n-ary relation types, while the remaining documents are annotated using a weakly-supervised labeling approach implemented with the Snorkel framework. To the best of our knowledge, this is the first German-language dataset that combines annotations for NER, EL and RE, and thus can be used for joint and multi-task learning of these fundamental information extraction tasks. We make MobIE public at https://github.com/dfki-nlp/mobie.","tags":[],"title":"MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain","type":"publication"},{"authors":["Dana Ruiter","Dietrich Klakow","Josef van Genabith","Cristina España i Bonet"],"categories":[],"content":"","date":1629158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629158400,"objectID":"4caebc3538cff2c57aa900184f540375","permalink":"https://cora4nlp.github.io/publication/ruiter-mtsummit-2021/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/publication/ruiter-mtsummit-2021/","section":"publication","summary":" For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili).","tags":[],"title":"Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages","type":"publication"},{"authors":["Jingyi Zhang","Josef van Genabith"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"d66dc327c8c04e6e89f64732e14f1281","permalink":"https://cora4nlp.github.io/publication/zhang-acl-2021-btba/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/zhang-acl-2021-btba/","section":"publication","summary":"Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.","tags":[],"title":"A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment","type":"publication"},{"authors":["Hongfei Xu","Qiuhui Liu","Josef van Genabith","Deyi Xiong"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"8818a80f83be6a871218b1151b230f7e","permalink":"https://cora4nlp.github.io/publication/xu-acl-2021-multi/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/xu-acl-2021-multi/","section":"publication","summary":"Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.","tags":[],"title":"Modeling Task-Aware {MIMO} Cardinality for Efficient Multilingual Neural Machine Translation","type":"publication"},{"authors":["Hongfei Xu","Qiuhui Liu","Josef van Genabith","Deyi Xiong","Meng Zhang"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"da5956c21f68f69b2aecb4d783e8ea39","permalink":"https://cora4nlp.github.io/publication/xu-acl-2021-lstm/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/xu-acl-2021-lstm/","section":"publication","summary":"One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is $O(n^2)$, increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.","tags":[],"title":"Multi-Head Highly Parallelized {LSTM} Decoder for Neural Machine Translation","type":"publication"},{"authors":["Stalin Varanasi","Saadullah Amin","Günter Neumann"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"258edd7fc992d8ad2253dbbfc10c99c7","permalink":"https://cora4nlp.github.io/publication/varanasi-emnlp-2021-autoeqa/","publishdate":"2021-09-14T00:00:00Z","relpermalink":"/publication/varanasi-emnlp-2021-autoeqa/","section":"publication","summary":"There  has  been  a  significant  progress  in  thefield of extractive question answering (EQA)in  the  recent  years.   However,  most  of  themrely on annotations of answer-spans in the cor-responding  passages.    In  this  work,  we  ad-dress  the  problem  of  EQA  when  no  annotations  are  present  for  the  answer  span,  i.e.,when the dataset contains only questions andcorresponding passages.  Our method is basedon auto-encoding of the question that performsa question answering (QA) task during encoding and a question generation (QG) task duringdecoding. Our method performs well in a zero-shot setting and can provide an additional lossto boost performance for EQA.","tags":[],"title":"AutoEQA: Auto-Encoding Questions for Extractive Question Answering","type":"publication"},{"authors":["Hongfei Xu","Josef van Genabith","Qiuhui Liu","Deyi Xiong"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"495a7d38737e53d09cf2807811800aa9","permalink":"https://cora4nlp.github.io/publication/xu-naacl-2021-prob/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/xu-naacl-2021-prob/","section":"publication","summary":"Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.","tags":[],"title":"Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers","type":"publication"},{"authors":["Saadullah Amin","Günter Neumann"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5da024c2c39f2a31d4830593a84db8d0","permalink":"https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/amin-eacl-2021-t2ner/","section":"publication","summary":"Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: https://github.com/suamin/t2ner.","tags":[],"title":"T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition","type":"publication"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"9a5121ae3f2dcac74af40e691db516cd","permalink":"https://cora4nlp.github.io/publication/rethmeier-arxiv-2021-primer/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/rethmeier-arxiv-2021-primer/","section":"publication","summary":"Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various application tasks. These pretraining methods are frequently extended with recurrence, adversarial or linguistic property masking, and more recently with contrastive learning objectives. Contrastive self-supervised training objectives enabled recent successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar. However, in NLP, automated creation of text input augmentations is still very challenging because a single token can invert the meaning of a sentence. For this reason, some contrastive NLP pretraining methods contrast over input-label pairs, rather than over input-input pairs, using methods from Metric Learning and Energy Based Models. In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks. We introduce key contrastive learning concepts with lessons learned from prior research and structure works by applications and cross-field relations. Finally, we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining.","tags":[],"title":"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina España i Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"eaca0924d0ff193f29619aba2bc5d44c","permalink":"https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-eacl-2021-menyo20k/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource Yorùbá--English (yo--en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook's M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.","tags":[],"title":"MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina España i Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"65cb3cbf09307d6c42ee87c9fd75571f","permalink":"https://cora4nlp.github.io/publication/adelani-mtsummit-2021/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-mtsummit-2021/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus with a special focus on clean orthography for Yorúbà-English with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality, we also analyze the effect of diacritics, a major characteristic of Yorúbà, in the training data. We investigate how and when this training condition affects the final quality and intelligibility of a translation. Our models outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2M (+9.1 BLEU) when translating to Yorúbà, setting a high quality benchmark for future research.","tags":[],"title":"The Effect of Domain and Diacritics in Yorúbà-English Neural Machine Translation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://cora4nlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://cora4nlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data. In this data protection policy we inform you about the processing of your personal data when visiting and using our web site.\nResponsible service provider German Research Center for Artificial Intelligence (DFKI) Phone: +49 (0)631 / 205 75-0, Email: info@dfki.de\nData Protection Officer Phone: +49 (0)631 / 205 75-0 Email: datenschutz@dfki.de\nIntended use  Provision of the information offering in the course of the public communication of the DFKI Establishment of contact and correspondence with visitors and users  Anonymous and protected use Visit and usage of our web site are anonymous. At our web site personal data are only collected to the technically necessary extent. The processed data will not be transmitted to any third parties or otherwise disclosed, except on the basis of concrete lawful obligations. Within our information offering we do not embed information or service offerings of third party providers.\nWhile using our web site the data transmission in the internet is being protected by a generally accepted secure encryption procedure and hence cannot easily be eavesdropped or tampered.\nAccess data Every time our website is accessed, usage, traffic and connection data are collected, temporarily stored in a log file and generally deleted after 90 days.\nThe following data is stored for each access/retrieval:\n IP address Transmitted user agent information (in particular type/version of web browser, operating system, etc.) Transmitted referrer information (URL of the referring page) Date and time of access/retrieval Submitted Access Method/Function Transmitted input values ​​(search terms, etc.) Accessed page or retrieved file Amount of data transferred Access/retrieval processing status  The processing of the access data is lawful because it is necessary for the purposes of the legitimate interests pursued by DFKI. The legitimate interests pursued by DFKI are the adaptation and optimisation of the information offering and the investigation, detection and prosecution of illegal activities in connection with the usage of our web site.\nThe stored data records can be statistically evaluated in order to adapt and optimize our web site to the needs of our visitors. Any techniques that offer the possibility to retrace the access characteristics of users (tracking) will not be applied. The creation of user profiles and automated decision-making based on it is precluded.\nThe stored data records are not attributable to specific persons. They are generally not being combined with other data sources. However, the stored data can be analysed and combined with other data sources, if we become aware of concrete indications of any illegal usage.\nCookies We use so-called cookies on our web site. Cookies are small files that are being stored by your web browser. The cookies used on our web site do not harm your computer and do not contain any malicious software. They offer a user-friendly and effective usage of our web site. We do not use cookies for marketing purposes.\nWe transmit so-called session cookies to your web browser. They are valid only for the duration of your visit on our web site and they do not have any meaning outside of our web site. The session cookies are needed in order to identify your session with a unique number during your visit and to transmit our contents in your preferred language. At the end of your visit the session cookies will be automatically deleted upon termination of your web browser.\nWe also transmit permanent cookies to your web browser with a validity period of at most 365 days. We are exclusively using these cookies in order to respect your settings for the type of presentation (normal, inverted) and for the font size. Furthermore, it will be recorded whether you’ve taken notice of the information about the usage of cookies in your web browser.\nYou can adjust your web browser such that you will be informed on setting cookies and allow cookies on an individual basis resp. exclude the acceptance of cookies for specific cases or generally. You also can adjust the automatic deletion of cookies upon termination of your web browser. Upon deactivation of cookies the functionality of our web site can be limited. In any case, our information offering is available to its full extent.\nSocial media We do not embed social media plug-ins on our web site. When you are visiting our web site no data are transmitted to social media services. Profiling by any third parties hence is precluded. You though have the option to change over to our information offerings on Facebook, Twitter and YouTube. For the usage of these services we refer to the data protection policies of the respective service providers. We are processing your personal data within the social networks insofar as you post contributions, send messages or otherwise communicate with us.\nCorrespondence You have the option to contact us by e-mail. We will use your e-mail address and other personal contact data for the correspondence with you. Due to lawful obligation every e-mail correspondence will be archived. Subject to our legitimate interests your e-mail address and other personal contact data can be stored in our contact data base. In this case you will receive a corresponding information on the processing of your contact data.\nAccess and Intervention Besides the information in this data protection policy you have the right of access to your personal data. To ensure fair data processing, you have the following rights:\n The right to rectification and completion of your personal data The right to erasure of your personal data The right to restriction of the processing of your personal data The right to object to the processing of your personal data on grounds related to your particular situation  To exercise these rights, please contact our data protection officer.\nRight to lodge a complaint You have the right to lodge a complaint with a supervisory authority if you consider that the processing of your personal data infringes statutory data protection regulations.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://cora4nlp.github.io/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"The German Research Center for Artificial Intelligence (Deutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI)) and its staff are committed to goal- and risk-oriented information privacy and the fundamental right to the protection of personal data.","tags":null,"title":"Data Protection Notice","type":"page"},{"authors":null,"categories":null,"content":"Responsible service provider Responsible for the content of the domain cora4nlp.github.io from the point of view of § 5 TMG:\nDeutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Krüger Helmut Ditzer Trippstadter Str. 122 67663 Kaiserslautern Germany Phone: +49 631 20575 0 Fax: +49 631 20575 5030 Email: info@dfki.de\nRegister Court: Amtsgericht Kaiserslautern Register Number: HRB 2313\nID-Number: DE 148 646 973\nThe person responsible for the editorial content of the domain cora4nlp.github.io of the German Research Center for Artificial Intelligence GmbH within the meaning of § 18 para. 2 MStV is:\nDr. Leonhard Hennig, Senior Researcher DFKI Lab Berlin Alt-Moabit 91c D-10559 Berlin Tel: +49 (0)30 / 238 95-0 Email: leonhard.hennig@dfki.de Website URL: www.dfki.de\nLegal notice concerning liability for proprietary content As a content provider in accordance with Section 7 (1) of the German Telemedia Act (Telemediengesetz), the Deutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI) is responsible for its own content that is used pursuant to the general laws. The DFKI endeavors to ensure that the information provided on this website is accurate and current. Nevertheless, errors and uncertainties cannot be entirely ruled out. For this reason, the DFKI undertakes no liability for ensuring that the provided information is current, accurate or complete, and is not responsible for its quality. The DFKI is not liable for material or immaterial damages caused directly or indirectly by the use or non-use of the offered information, or by the use of erroneous and incomplete information, unless willful or grossly negligent fault can be demonstrated. This also applies with respect to software or data provided for download. The DFKI reserves the right to modify, expand or delete parts of the website or the entire website without separate announcement, or to cease publication temporarily or definitively.\nLegal notices for third party content and references to external websites As a service provider, we are responsible for our content on these pages in accordance with general law (§ 7 (1) TMG). According to § 8 – 10 TMG, as a service provider, we are not obliged to monior third-party information that is transmitted or stored, or to investigate circumstances that indicate illegal activity. Obligations to remove or block the use of information according to general laws remain unaffected. However, liability in this regard is only possible form the point in time at which we become aware of a specific legal violation. If we become aware of legal violations, we will remove this content immediately. Cross- references (“links”) to the content providers are to be distinguished from own content. Our offer contains links to external third party website. The respective provider is always responsible for the content of the linked external pages. We cannot accept any liability for the content for the content of the linked pages. After this external content was checked by the DFKI when the link was recenty set, it was checked whether there were any preliminary legal violations. At the time oft he review, no legal violations were apparent. However, it cannot be ruled out that the content may be changed afterwards by teh resprective providers. A permanent control oft he content of the linked pages is not reasonable without ecidence of a violation of the law. The DFKI does not constantly check the content to which it refers in ist offer for changes that could justify a new responsibility. If you are oft he option that the linked external pages violate applicable law or have otherwie inappropriate content, please inform us directly: info@dfki.de. Should the DFKI discover or receive a message that an external offer to whicht is has linked triggers civil or criminal liability, the DFKI will remove the link to this offer.\nLegal notice concerning copyright The layout of the homepage, the graphics used and other content on the DFKI website are protected by copyright. The reproduction, processing, distribution and any type of use outside the boundaries of copyright law require the written approval of the DFKI (in writing). Insofar as any content on this page was not prepared by the DFKI, the copyrights of third parties will be observed. If you become aware of a copyright breach nevertheless, please inform us accordingly. Upon becoming aware of relevant legal breaches, the DFKI will remove such content immediately.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://cora4nlp.github.io/terms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/terms/","section":"","summary":"Responsible service provider Responsible for the content of the domain cora4nlp.github.io from the point of view of § 5 TMG:\nDeutsches Forschungszentrum für Künstliche Intelligenz GmbH (DFKI) Management: Prof. Dr. Antonio Krüger Helmut Ditzer Trippstadter Str.","tags":null,"title":"Legal Information","type":"page"}]