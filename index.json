[{"authors":["josef-van-genabith"],"categories":null,"content":"","date":1629158400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1629158400,"objectID":"d654f4176667d586adc2001840628351","permalink":"https://cora4nlp.github.io/author/josef-van-genabith/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/josef-van-genabith/","section":"authors","summary":"","tags":["Principal Investigators"],"title":"Josef van Genabith","type":"authors"},{"authors":["jingyi-zhang"],"categories":null,"content":"","date":1627862400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1627862400,"objectID":"33a41c6adcffdbccfd2a15d0f3501993","permalink":"https://cora4nlp.github.io/author/jingyi-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingyi-zhang/","section":"authors","summary":"","tags":["Researchers"],"title":"Jingyi Zhang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617235200,"objectID":"ac12c3e849a87511f0b8932e2a700083","permalink":"https://cora4nlp.github.io/author/saadullah-amin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/saadullah-amin/","section":"authors","summary":"","tags":["PhD Students"],"title":"Saadullah Amin","type":"authors"},{"authors":["arne-binder"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db472dfe584699b02c3fb0b542f4efba","permalink":"https://cora4nlp.github.io/author/arne-binder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/arne-binder/","section":"authors","summary":"","tags":["PhD Students"],"title":"Arne Binder","type":"authors"},{"authors":null,"categories":null,"content":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"55f7fc0a0becc469231bd11edf9d90c1","permalink":"https://cora4nlp.github.io/author/christoph-alt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christoph-alt/","section":"authors","summary":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["cristina-espana"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"03b0e63928d4828562e76d3d9421b170","permalink":"https://cora4nlp.github.io/author/cristina-espana-i-bonet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/cristina-espana-i-bonet/","section":"authors","summary":"","tags":["Researchers"],"title":"Cristina España i Bonet","type":"authors"},{"authors":["david-harbecke"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57423a933a38af75c718059324719b6e","permalink":"https://cora4nlp.github.io/author/david-harbecke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-harbecke/","section":"authors","summary":"","tags":["PhD Students"],"title":"David Harbecke","type":"authors"},{"authors":["guenter-neumann"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"927589e6c78af66aca57466cc9bf166b","permalink":"https://cora4nlp.github.io/author/guenter-neumann/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/guenter-neumann/","section":"authors","summary":"","tags":["Principal Investigators"],"title":"Guenter Neumann","type":"authors"},{"authors":["leonhard-hennig"],"categories":null,"content":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available. As a project lead, I\u0026rsquo;ve managed various national research projects, such as Smart Data Web, PLASS, DAYSTREAM, and the DFKI part in the Berlin Big Data Center, as well as industry-funded projects, e.g. for Deutsche Telekom and Lenovo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76acb2a1dbfa728042427546fca4cab6","permalink":"https://cora4nlp.github.io/author/leonhard-hennig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leonhard-hennig/","section":"authors","summary":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available.","tags":["Researchers"],"title":"Leonhard Hennig","type":"authors"},{"authors":["nils-rethmeier"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5fa9f4aadfb6bfa3e5e6ad75c3611835","permalink":"https://cora4nlp.github.io/author/nils-rethmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nils-rethmeier/","section":"authors","summary":"","tags":["PhD Students"],"title":"Nils Rethmeier","type":"authors"},{"authors":["sebastian-moeller"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"881850e20e8b4f43d7ab901d58ccc663","permalink":"https://cora4nlp.github.io/author/sebastian-moller/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sebastian-moller/","section":"authors","summary":"","tags":["Principal Investigators"],"title":"Sebastian Möller","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"05deb29b20577d1ba94a0381a241684f","permalink":"https://cora4nlp.github.io/author/stalin-varanasi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/stalin-varanasi/","section":"authors","summary":"","tags":["PhD Students"],"title":"Stalin Varanasi","type":"authors"},{"authors":["tatiana-anikina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec7d5b5ff62925a22fe4525ccbc2a55d","permalink":"https://cora4nlp.github.io/author/tatiana-anikina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tatiana-anikina/","section":"authors","summary":"","tags":["PhD Students"],"title":"Tatiana Anikina","type":"authors"},{"authors":["Leonhard Hennig and Phuc Tran Truong and Aleksandra Gabryszak"],"categories":[],"content":"","date":1630972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630972800,"objectID":"7b98766f43a9e140f9b34317ab45d6c9","permalink":"https://cora4nlp.github.io/publication/hennig-konvens-2021-mobie/","publishdate":"2021-08-11T00:00:00Z","relpermalink":"/publication/hennig-konvens-2021-mobie/","section":"publication","summary":"We present MobIE, a German-language dataset, which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,232 social media texts and traffic reports with 91K tokens, and contains 20.5K annotated entities, 13.1K of which are linked to a knowledge base. A subset of the dataset is human-annotated with seven mobility-related, n-ary relation types, while the remaining documents are annotated using a weakly-supervised labeling approach implemented with the Snorkel framework. To the best of our knowledge, this is the first German-language dataset that combines annotations for NER, EL and RE, and thus can be used for joint and multi-task learning of these fundamental information extraction tasks. We make MobIE public at https://github.com/dfki-nlp/mobie.","tags":[],"title":"MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain","type":"publication"},{"authors":["Dana Ruiter","Dietrich Klakow","Josef van Genabith","Cristina España-Bonet"],"categories":[],"content":"","date":1629158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629158400,"objectID":"4caebc3538cff2c57aa900184f540375","permalink":"https://cora4nlp.github.io/publication/ruiter-mtsummit-2021/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/publication/ruiter-mtsummit-2021/","section":"publication","summary":" For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation (UMT) exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT (SSNMT) identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili).","tags":[],"title":"Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages","type":"publication"},{"authors":["Jingyi Zhang","Josef van Genabith"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"d66dc327c8c04e6e89f64732e14f1281","permalink":"https://cora4nlp.github.io/publication/zhang-acl-2021-btba/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/zhang-acl-2021-btba/","section":"publication","summary":"Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.","tags":[],"title":"A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment","type":"publication"},{"authors":["Hongfei Xu","Qiuhui Liu","Josef van Genabith","Deyi Xiong"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"8818a80f83be6a871218b1151b230f7e","permalink":"https://cora4nlp.github.io/publication/xu-acl-2021-multi/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/xu-acl-2021-multi/","section":"publication","summary":"Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.","tags":[],"title":"Modeling Task-Aware {MIMO} Cardinality for Efficient Multilingual Neural Machine Translation","type":"publication"},{"authors":["Hongfei Xu","Qiuhui Liu","Josef van Genabith","Deyi Xiong","Meng Zhang"],"categories":[],"content":"","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627862400,"objectID":"da5956c21f68f69b2aecb4d783e8ea39","permalink":"https://cora4nlp.github.io/publication/xu-acl-2021-lstm/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/publication/xu-acl-2021-lstm/","section":"publication","summary":"One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is $O(n^2)$, increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.","tags":[],"title":"Multi-Head Highly Parallelized {LSTM} Decoder for Neural Machine Translation","type":"publication"},{"authors":["Hongfei Xu","Josef van Genabith","Qiuhui Liu","Deyi Xiong"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"495a7d38737e53d09cf2807811800aa9","permalink":"https://cora4nlp.github.io/publication/xu-naacl-2021-prob/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/xu-naacl-2021-prob/","section":"publication","summary":"Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.","tags":[],"title":"Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers","type":"publication"},{"authors":["Saadullah Amin","Günter Neumann"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5da024c2c39f2a31d4830593a84db8d0","permalink":"https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/amin-eacl-2021-t2ner/","section":"publication","summary":"Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: https://github.com/suamin/t2ner.","tags":[],"title":"T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition","type":"publication"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"9a5121ae3f2dcac74af40e691db516cd","permalink":"https://cora4nlp.github.io/publication/rethmeier-arxiv-2021-primer/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/rethmeier-arxiv-2021-primer/","section":"publication","summary":"Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various application tasks. These pretraining methods are frequently extended with recurrence, adversarial or linguistic property masking, and more recently with contrastive learning objectives. Contrastive self-supervised training objectives enabled recent successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar. However, in NLP, automated creation of text input augmentations is still very challenging because a single token can invert the meaning of a sentence. For this reason, some contrastive NLP pretraining methods contrast over input-label pairs, rather than over input-input pairs, using methods from Metric Learning and Energy Based Models. In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks. We introduce key contrastive learning concepts with lessons learned from prior research and structure works by applications and cross-field relations. Finally, we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining.","tags":[],"title":"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina España-Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"eaca0924d0ff193f29619aba2bc5d44c","permalink":"https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-eacl-2021-menyo20k/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource Yorùbá--English (yo--en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook's M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.","tags":[],"title":"MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina España-Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"65cb3cbf09307d6c42ee87c9fd75571f","permalink":"https://cora4nlp.github.io/publication/adelani-mtsummit-2021/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-mtsummit-2021/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due to lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus with a special focus on clean orthography for Yorúbà-English with standardized train-test splits for benchmarking. We provide several neural MT benchmarks and compare them to the performance of popular pre-trained (massively multilingual) MT models both for the heterogeneous test set and its subdomains. Since these pre-trained models use huge amounts of data with uncertain quality, we also analyze the effect of diacritics, a major characteristic of Yorúbà, in the training data. We investigate how and when this training condition affects the final quality and intelligibility of a translation. Our models outperform massively multilingual models such as Google (+8.7 BLEU) and Facebook M2M (+9.1 BLEU) when translating to Yorúbà, setting a high quality benchmark for future research.","tags":[],"title":"The Effect of Domain and Diacritics in Yorúbà-English Neural Machine Translation","type":"publication"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603238400,"objectID":"26d5e8179dc1bbe4f4de983b2889ea75","permalink":"https://cora4nlp.github.io/publication/rethmeier-arxiv-2020-longtail/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/publication/rethmeier-arxiv-2020-longtail/","section":"publication","summary":"For natural language processing 'text-to-text' tasks, the prevailing approaches heavily rely on pretraining large self-supervised models on increasingly larger 'task-external' data. Transfer learning from high-resource pretraining works well, but research has focused on settings with very large data and compute requirements, while the potential of efficient low-resource learning, without large 'task-external' pretraining, remains under-explored. In this work, we evaluate against three core challenges for resource efficient learning. Namely, we analyze: (1) pretraining data (X) efficiency; (2) zero to few-shot label (Y) efficiency; and (3) long-tail generalization, since long-tail preservation has been linked to algorithmic fairness and because data in the tail is limited by definition. To address these challenges, we propose a data and compute efficient self-supervised, contrastive text encoder, pretrained on 60MB of 'task-internal' text data, and compare it to RoBERTa, which was pretrained on 160GB of 'task-external' text. We find our method outperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa's fine-tuning time.","tags":[],"title":"Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://cora4nlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://cora4nlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]