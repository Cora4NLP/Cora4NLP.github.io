[{"authors":null,"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617235200,"objectID":"ac12c3e849a87511f0b8932e2a700083","permalink":"https://cora4nlp.github.io/author/saadullah-amin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/saadullah-amin/","section":"authors","summary":"","tags":["PhD Students"],"title":"Saadullah Amin","type":"authors"},{"authors":["arne-binder"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db472dfe584699b02c3fb0b542f4efba","permalink":"https://cora4nlp.github.io/author/arne-binder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/arne-binder/","section":"authors","summary":"","tags":["PhD Students"],"title":"Arne Binder","type":"authors"},{"authors":null,"categories":null,"content":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"55f7fc0a0becc469231bd11edf9d90c1","permalink":"https://cora4nlp.github.io/author/christoph-alt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christoph-alt/","section":"authors","summary":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["david-harbecke"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57423a933a38af75c718059324719b6e","permalink":"https://cora4nlp.github.io/author/david-harbecke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-harbecke/","section":"authors","summary":"","tags":["PhD Students"],"title":"David Harbecke","type":"authors"},{"authors":["leonhard-hennig"],"categories":null,"content":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available. As a project lead, I\u0026rsquo;ve managed various national research projects, such as Smart Data Web, PLASS, DAYSTREAM, and the DFKI part in the Berlin Big Data Center, as well as industry-funded projects, e.g. for Deutsche Telekom and Lenovo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76acb2a1dbfa728042427546fca4cab6","permalink":"https://cora4nlp.github.io/author/leonhard-hennig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leonhard-hennig/","section":"authors","summary":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available.","tags":["Researchers"],"title":"Leonhard Hennig","type":"authors"},{"authors":["nils-rethmeier"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5fa9f4aadfb6bfa3e5e6ad75c3611835","permalink":"https://cora4nlp.github.io/author/nils-rethmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nils-rethmeier/","section":"authors","summary":"","tags":["PhD Students"],"title":"Nils Rethmeier","type":"authors"},{"authors":["tatiana-anikina"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec7d5b5ff62925a22fe4525ccbc2a55d","permalink":"https://cora4nlp.github.io/author/tatiana-anikina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tatiana-anikina/","section":"authors","summary":"","tags":["PhD Students"],"title":"Tatiana Anikina","type":"authors"},{"authors":["Hongfei Xu","Josef van Genabith","Qiuhui Liu","Deyi Xiong"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"495a7d38737e53d09cf2807811800aa9","permalink":"https://cora4nlp.github.io/publication/xu-naacl-2021-prob/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/xu-naacl-2021-prob/","section":"publication","summary":"Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.","tags":[],"title":"Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers","type":"publication"},{"authors":["Saadullah Amin","GÃ¼nter Neumann"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5da024c2c39f2a31d4830593a84db8d0","permalink":"https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/amin-eacl-2021-t2ner/","section":"publication","summary":"Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: https://github.com/suamin/t2ner.","tags":[],"title":"T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition","type":"publication"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"9a5121ae3f2dcac74af40e691db516cd","permalink":"https://cora4nlp.github.io/publication/rethmeier-arxiv-2021-primer/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/rethmeier-arxiv-2021-primer/","section":"publication","summary":"Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various application tasks. These pretraining methods are frequently extended with recurrence, adversarial or linguistic property masking, and more recently with contrastive learning objectives. Contrastive self-supervised training objectives enabled recent successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar. However, in NLP, automated creation of text input augmentations is still very challenging because a single token can invert the meaning of a sentence. For this reason, some contrastive NLP pretraining methods contrast over input-label pairs, rather than over input-input pairs, using methods from Metric Learning and Energy Based Models. In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks. We introduce key contrastive learning concepts with lessons learned from prior research and structure works by applications and cross-field relations. Finally, we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining.","tags":[],"title":"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina EspaÃ±a-Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"eaca0924d0ff193f29619aba2bc5d44c","permalink":"https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-eacl-2021-menyo20k/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource YorÃ¹bÃ¡--English (yo--en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook's M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.","tags":[],"title":"MENYO-20k: A Multi-domain English-YorÃ¹bÃ¡ Corpus for Machine Translation and Domain Adaptation","type":"publication"},{"authors":["N. Rethmeier","I. Augenstein"],"categories":[],"content":"","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603238400,"objectID":"26d5e8179dc1bbe4f4de983b2889ea75","permalink":"https://cora4nlp.github.io/publication/rethmeier-arxiv-2020-longtail/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/publication/rethmeier-arxiv-2020-longtail/","section":"publication","summary":"For natural language processing 'text-to-text' tasks, the prevailing approaches heavily rely on pretraining large self-supervised models on increasingly larger 'task-external' data. Transfer learning from high-resource pretraining works well, but research has focused on settings with very large data and compute requirements, while the potential of efficient low-resource learning, without large 'task-external' pretraining, remains under-explored. In this work, we evaluate against three core challenges for resource efficient learning. Namely, we analyze: (1) pretraining data (X) efficiency; (2) zero to few-shot label (Y) efficiency; and (3) long-tail generalization, since long-tail preservation has been linked to algorithmic fairness and because data in the tail is limited by definition. To address these challenges, we propose a data and compute efficient self-supervised, contrastive text encoder, pretrained on 60MB of 'task-internal' text data, and compare it to RoBERTa, which was pretrained on 160GB of 'task-external' text. We find our method outperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa's fine-tuning time.","tags":[],"title":"Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://cora4nlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://cora4nlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]