[{"authors":["arne-binder"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"db472dfe584699b02c3fb0b542f4efba","permalink":"https://cora4nlp.github.io/author/arne-binder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/arne-binder/","section":"authors","summary":"","tags":["PhD Students"],"title":"Arne Binder","type":"authors"},{"authors":null,"categories":null,"content":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"55f7fc0a0becc469231bd11edf9d90c1","permalink":"https://cora4nlp.github.io/author/christoph-alt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christoph-alt/","section":"authors","summary":"I am a postdoctoral (senior) researcher at the German Research Center for AI (DFKI), Berlin. My goal is to solve complex natural language understanding problems with limited (labeled) data, e.g., via transfer-learning, multi-task learning, few- and zero-shot learning.","tags":null,"title":"Christoph Alt","type":"authors"},{"authors":["david-harbecke"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57423a933a38af75c718059324719b6e","permalink":"https://cora4nlp.github.io/author/david-harbecke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-harbecke/","section":"authors","summary":"","tags":["PhD Students"],"title":"David Harbecke","type":"authors"},{"authors":["leonhard-hennig"],"categories":null,"content":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available. As a project lead, I\u0026rsquo;ve managed various national research projects, such as Smart Data Web, PLASS, DAYSTREAM, and the DFKI part in the Berlin Big Data Center, as well as industry-funded projects, e.g. for Deutsche Telekom and Lenovo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76acb2a1dbfa728042427546fca4cab6","permalink":"https://cora4nlp.github.io/author/leonhard-hennig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leonhard-hennig/","section":"authors","summary":"I\u0026rsquo;m a senior researcher and project manager at the DFKI Speech \u0026amp; Language Technology Lab. I\u0026rsquo;m interested in applying machine learning techniques to computational linguistics problems, such as information extraction and summarization, and making these work on real-world, domain-specific, noisy data in low-resource settings, where little or no language resources are readily available.","tags":["Researchers"],"title":"Leonhard Hennig","type":"authors"},{"authors":["nils-rethmeier"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5fa9f4aadfb6bfa3e5e6ad75c3611835","permalink":"https://cora4nlp.github.io/author/nils-rethmeier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nils-rethmeier/","section":"authors","summary":"","tags":["PhD Students"],"title":"Nils Rethmeier","type":"authors"},{"authors":["Saadullah Amin","Günter Neumann"],"categories":[],"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"5da024c2c39f2a31d4830593a84db8d0","permalink":"https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/amin-eacl-2021-t2ner/","section":"publication","summary":"Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: https://github.com/suamin/t2ner.","tags":[],"title":"T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition","type":"publication"},{"authors":["David I. Adelani","Dana Ruiter","Jesujoba O. Alabi","Damilola Adebonojo","Adesina Ayeni","Mofe Adeyemi","Ayodele Awokoya","Cristina España-Bonet"],"categories":[],"content":"","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"eaca0924d0ff193f29619aba2bc5d44c","permalink":"https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/publication/adelani-eacl-2021-menyo20k/","section":"publication","summary":"Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource Yorùbá--English (yo--en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook's M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.","tags":[],"title":"MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://cora4nlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://cora4nlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]