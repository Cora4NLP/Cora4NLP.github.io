<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Christoph Alt"><meta name=description content="Cora4NLP project page."><link rel=alternate hreflang=en-us href=https://cora4nlp.github.io/><meta name=theme-color content="#3f51b5"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.min.aee9166cce4168356878710aa1039ff5.css><link rel=alternate href=/index.xml type=application/rss+xml title=Cora4NLP><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://cora4nlp.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Cora4NLP"><meta property="og:url" content="https://cora4nlp.github.io/"><meta property="og:title" content="Cora4NLP"><meta property="og:description" content="Cora4NLP project page."><meta property="og:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2021-04-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","url":"https://cora4nlp.github.io"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cora4nlp.github.io","name":"Cora4NLP","logo":"https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png","address":{"@type":"PostalAddress","streetAddress":"Alt-Moabit 91c","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10559","addressCountry":"DE"},"url":"https://cora4nlp.github.io"}</script><title>Cora4NLP</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Cora4NLP</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Cora4NLP</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Contextual Reasoning and Adaptation for Natural Language Processing</h1></div><div class="col-12 col-lg-8"><p>Language is implicit — it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains — and transfer to, and between, both. Current language understanding methods, however, focus on high resource languages and domains, use little to no context, and assume static data, task, and target distributions.</p><p>The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project <a href=https://www.deeplee.de target=_blank rel=noopener>DEEPLEE</a> and is carried out jointly between DFKI&rsquo;s language technology research departments in <a href=https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/ target=_blank rel=noopener>Berlin</a> and <a href=https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/ target=_blank rel=noopener>Saarbrücken</a>. Specifically, our goal is to develop natural language understanding methods that enable:</p><ul><li>reasoning over broader co- and contexts</li><li>efficient adaptation to novel and/or low resource contexts</li><li>continual adaptation to, and generalization over, evolving contexts</li></ul><p>Cora4NLP is funded by the German Federal Ministry of Education and Research (BMBF) under funding code 01IW20010.</p></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Latest News</h1></div><div class="col-12 col-lg-8"></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/hongfei-xu/>Hongfei Xu</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span>, <span><a href=/author/qiuhui-liu/>Qiuhui Liu</a></span>, <span><a href=/author/deyi-xiong/>Deyi Xiong</a></span></div><span class=article-date>April 2021</span>
<span class=middot-divider></span><span class=pub-publication>NAACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/xu-naacl-2021-prob/>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</a></h3><a href=/publication/xu-naacl-2021-prob/ class=summary-link><div class=article-style><p>Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2021.naacl-main.7.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/xu-naacl-2021-prob/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/gunter-neumann/>Günter Neumann</a></span></div><span class=article-date>April 2021</span>
<span class=middot-divider></span><span class=pub-publication>EACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/amin-eacl-2021-t2ner/>T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition</a></h3><a href=/publication/amin-eacl-2021-t2ner/ class=summary-link><div class=article-style><p>Recent advances in deep transformer models have achieved state-of-the-art in several natural language processing (NLP) tasks, whereas named entity recognition (NER) has traditionally benefited from long-short term memory (LSTM) networks. In this work, we present a Transformers based Transfer Learning framework for Named Entity Recognition (T2NER) created in PyTorch for the task of NER with deep transformer models. The framework is built upon the Transformers library as the core modeling engine and supports several transfer learning scenarios from sequential transfer to domain adaptation, multi-task learning, and semi-supervised learning. It aims to bridge the gap between the algorithmic advances in these areas by combining them with the state-of-the-art in transformer models to provide a unified platform that is readily extensible and can be used for both the transfer learning research in NER, and for real-world applications. The framework is available at: <a href=https://github.com/suamin/t2ner>https://github.com/suamin/t2ner</a>.</p></div></a><div class=btn-links><button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/amin-eacl-2021-t2ner/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/n.-rethmeier/>N. Rethmeier</a></span>, <span><a href=/author/i.-augenstein/>I. Augenstein</a></span></div><span class=article-date>March 2021</span>
<span class=middot-divider></span><span class=pub-publication>arXiv</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/rethmeier-arxiv-2021-primer/>A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives</a></h3><a href=/publication/rethmeier-arxiv-2021-primer/ class=summary-link><div class=article-style><p>Modern natural language processing (NLP) methods employ self-supervised pretraining objectives such as masked language modeling to boost the performance of various application tasks. These pretraining methods are frequently extended with recurrence, adversarial or linguistic property masking, and more recently with contrastive learning objectives. Contrastive self-supervised training objectives enabled recent successes in image representation pretraining by learning to contrast input-input pairs of augmented images as either similar or dissimilar. However, in NLP, automated creation of text input augmentations is still very challenging because a single token can invert the meaning of a sentence. For this reason, some contrastive NLP pretraining methods contrast over input-label pairs, rather than over input-input pairs, using methods from Metric Learning and Energy Based Models. In this survey, we summarize recent self-supervised and supervised contrastive NLP pretraining methods and describe where they are used to improve language modeling, few or zero-shot learning, pretraining data-efficiency and specific NLP end-tasks. We introduce key contrastive learning concepts with lessons learned from prior research and structure works by applications and cross-field relations. Finally, we point to open challenges and future directions for contrastive NLP to encourage bringing contrastive NLP pretraining closer to recent successes in image representation pretraining.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2102.12982 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/rethmeier-arxiv-2021-primer/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/david-i.-adelani/>David I. Adelani</a></span>, <span><a href=/author/dana-ruiter/>Dana Ruiter</a></span>, <span><a href=/author/jesujoba-o.-alabi/>Jesujoba O. Alabi</a></span>, <span><a href=/author/damilola-adebonojo/>Damilola Adebonojo</a></span>, <span><a href=/author/adesina-ayeni/>Adesina Ayeni</a></span>, <span><a href=/author/mofe-adeyemi/>Mofe Adeyemi</a></span>, <span><a href=/author/ayodele-awokoya/>Ayodele Awokoya</a></span>, <span><a href=/author/cristina-espana-bonet/>Cristina España-Bonet</a></span></div><span class=article-date>March 2021</span>
<span class=middot-divider></span><span class=pub-publication>AfricaNLP 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/adelani-eacl-2021-menyo20k/>MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation</a></h3><a href=/publication/adelani-eacl-2021-menyo20k/ class=summary-link><div class=article-style><p>Massively multilingual machine translation (MT) has shown impressive capabilities, including zero and few-shot translation between low-resource language pairs. However, these models are often evaluated on high-resource languages with the assumption that they generalize to low-resource ones. The difficulty of evaluating MT models on low-resource pairs is often due the lack of standardized evaluation datasets. In this paper, we present MENYO-20k, the first multi-domain parallel corpus for the low-resource Yorùbá&ndash;English (yo&ndash;en) language pair with standardized train-test splits for benchmarking. We provide several neural MT (NMT) benchmarks on this dataset and compare to the performance of popular pre-trained (massively multilingual) MT models, showing that, in almost all cases, our simple benchmarks outperform the pre-trained MT models. A major gain of BLEU +9.9 and +8.6 (en2yo) is achieved in comparison to Facebook&rsquo;s M2M-100 and Google multilingual NMT respectively when we use MENYO-20k to fine-tune generic models.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2103.08647 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/adelani-eacl-2021-menyo20k/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/n.-rethmeier/>N. Rethmeier</a></span>, <span><a href=/author/i.-augenstein/>I. Augenstein</a></span></div><span class=article-date>October 2020</span>
<span class=middot-divider></span><span class=pub-publication>arXiv</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/rethmeier-arxiv-2020-longtail/>Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data</a></h3><a href=/publication/rethmeier-arxiv-2020-longtail/ class=summary-link><div class=article-style><p>For natural language processing &lsquo;text-to-text&rsquo; tasks, the prevailing approaches heavily rely on pretraining large self-supervised models on increasingly larger &lsquo;task-external&rsquo; data. Transfer learning from high-resource pretraining works well, but research has focused on settings with very large data and compute requirements, while the potential of efficient low-resource learning, without large &lsquo;task-external&rsquo; pretraining, remains under-explored. In this work, we evaluate against three core challenges for resource efficient learning. Namely, we analyze: (1) pretraining data (X) efficiency; (2) zero to few-shot label (Y) efficiency; and (3) long-tail generalization, since long-tail preservation has been linked to algorithmic fairness and because data in the tail is limited by definition. To address these challenges, we propose a data and compute efficient self-supervised, contrastive text encoder, pretrained on 60MB of &lsquo;task-internal&rsquo; text data, and compare it to RoBERTa, which was pretrained on 160GB of &lsquo;task-external&rsquo; text. We find our method outperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa&rsquo;s fine-tuning time.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2010.01061 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/rethmeier-arxiv-2020-longtail/cite.bib>
Cite</button></div></div></div></div></div></section><section id=sponsored class="home-section wg-blank"><div class=home-section-bg style=background-color:#fff></div><div class=container><div class="row justify-content-center"><div class=col-12><figure><a data-fancybox href=/media/bmbf_sponsored_and_logo_horizontal.png><img src=/media/bmbf_sponsored_and_logo_horizontal.png alt="Sponsored by the Federal Ministry of Education and Research"></a></figure></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© Cora4NLP 2021</p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=false;</script><script src=/js/wowchemy.min.1521bbf8c12bfed32e2e1938f0a7fc26.js></script></body></html>