<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=description content="Cora4NLP project page."><link rel=alternate hreflang=en-us href=https://cora4nlp.github.io/><meta name=theme-color content="#3f51b5"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.min.aee9166cce4168356878710aa1039ff5.css><link rel=alternate href=/index.xml type=application/rss+xml title=Cora4NLP><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://cora4nlp.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Cora4NLP"><meta property="og:url" content="https://cora4nlp.github.io/"><meta property="og:title" content="Cora4NLP"><meta property="og:description" content="Cora4NLP project page."><meta property="og:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-09-27T10:33:03+02:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","url":"https://cora4nlp.github.io"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cora4nlp.github.io","name":"Cora4NLP","logo":"https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png","address":{"@type":"PostalAddress","streetAddress":"Alt-Moabit 91c","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10559","addressCountry":"DE"},"url":"https://cora4nlp.github.io"}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#3f51b5","text":"rgb(255, 255, 255)"},"button":{"background":"rgb(255, 255, 255)","text":"#3f51b5"}},"theme":"classic","content":{"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"Got it!","link":"Learn more","href":"/privacy/"}})});</script><title>Cora4NLP</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Cora4NLP</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Cora4NLP</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#news data-target=#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Contextual Reasoning and Adaptation for Natural Language Processing</h1></div><div class="col-12 col-lg-8"><p>Language is implicit — it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains — and transfer to, and between, both. Current language understanding methods, however, focus on high resource languages and domains, use little to no context, and assume static data, task, and target distributions.</p><p>The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project <a href=https://www.deeplee.de target=_blank rel=noopener>DEEPLEE</a> and is carried out jointly between DFKI&rsquo;s language technology research departments in <a href=https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/ target=_blank rel=noopener>Berlin</a> and <a href=https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/ target=_blank rel=noopener>Saarbrücken</a>. Specifically, our goal is to develop natural language understanding methods that enable:</p><ul><li>reasoning over broader co- and contexts</li><li>efficient adaptation to novel and/or low resource contexts</li><li>continual adaptation to, and generalization over, evolving contexts</li></ul><p>Cora4NLP is funded by the German Federal Ministry of Education and Research (BMBF) under funding code 01IW20010.</p></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Latest News</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><span class=article-date>Last updated on
Aug 17, 2023</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/konvens2023/>1 paper to be presented at KONVENS 2023</a></h3><a href=/post/konvens2023/ class=summary-link><div class=article-style><p>One paper from Cora4NLP researchers has been accepted for publication at KONVENS 2023, the 19th German Conference on Natural Language Processing. The conference will take place in Ingolstadt, Germany, from Sep 18th to Sep 22nd, 2023.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>May 8, 2023</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/acl2023/>2 papers to be presented at ACL 2023</a></h3><a href=/post/acl2023/ class=summary-link><div class=article-style><p>Two papers from Cora4NLP researchers have been accepted for publication at ACL 2023, the 61st Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Toronto, Canada, from Jul 9th through July 14th, 2023.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Feb 23, 2023</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/eacl2023/>2 papers accepted at EACL 2023</a></h3><a href=/post/eacl2023/ class=summary-link><div class=article-style><p>We are happy to announce that two papers from Cora4NLP members have been accepted for publication at the 17th Conference of the European Chapter of the Association for Computational Linguistics. The conference will take place from May2nd to May 6th, 2023.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Dec 6, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/conll2022/>1 paper to be presented at CoNLL 2022</a></h3><a href=/post/conll2022/ class=summary-link><div class=article-style><p>One paper from Cora4NLP authors has been accepted for publication at CoNLL 2022, the SIGNLL Conference on Computational Natural Language Learning. The conference is co-located with EMNLP 2022 and planned to be a hybrid meeting.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Oct 14, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/emnlp2022/>2 papers to be presented at EMNLP 2022</a></h3><a href=/post/emnlp2022/ class=summary-link><div class=article-style><p>Two papers from Cora4NLP authors have been accepted for publication at EMNLP 2022, the 2022 Conference on Empirical Methods in Natural Language Processing. The conference is planned to be a hybrid meeting and will take place in Abu Dhabi, from Dec 7th to Dec 11th, 2022.</p></div></a></div><div class=see-all><a href=/post/>See all posts
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/hongyang-chang/>Hongyang Chang</a></span>, <span><a href=/author/hongfei-xu/>Hongfei Xu</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span>, <span><a href=/author/deyi-xiong/>Deyi Xiong</a></span>, <span><a href=/author/hongying-zan/>Hongying Zan</a></span></div><span class=article-date>September 2023</span>
<span class=middot-divider></span><span class=pub-publication>TASLP</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/taslp2023-xu-joinerbart/>JoinER-BART: Joint Entity and Relation Extraction with Constrained Decoding, Representation Reuse and Fusion</a></h3><a href=/publication/taslp2023-xu-joinerbart/ class=summary-link><div class=article-style><p>Joint Entity and Relation Extraction (JERE) is an important research direction in Information Extraction (IE). Given the surprising performance with fine-tuning of pre-trained BERT in a wide range of NLP tasks, nowadays most studies for JERE are based on the BERT model. Rather than predicting a simple tag for each word, these approaches are usually forced to design complex tagging schemes, as they may have to extract entity-relation pairs which may overlap with others from the same sequence of word representations in a sentence. Recently, sequence-to-sequence (seq2seq) pre-trained BART models show better performance than BERT models in many NLP tasks. Importantly, a seq2seq BART model can simply generate sequences of (many) entity-relation triplets with its decoder, rather than just tag input words. In this paper, we present a new generative JERE framework based on pre-trained BART. Different from the basic seq2seq BART architecture: 1) our framework employs a constrained classifier which only predicts either a token of the input sentence or a relation in each decoding step, and 2) we reuse representations from the pre-trained BART encoder in the classifier instead of a newly trained weight matrix, as this better utilizes the knowledge of the pre-trained model and context-aware representations for classification, and empirically leads to better performance. In our experiments on the widely studied NYT and WebNLG datasets, we show that our approach outperforms previous studies and establishes a new state-of-the-art (92.91 and 91.37 F1 respectively in exact match evaluation).</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10236558" target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/taslp2023-xu-joinerbart/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://doi.org/10.1109/TASLP.2023.3310879 target=_blank rel=noopener>DOI</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/mohammed-bin-sumait/>Mohammed Bin Sumait</a></span>, <span><a href=/author/aleksandra-gabryszak/>Aleksandra Gabryszak</a></span>, <span><a href=/author/leonhard-hennig/>Leonhard Hennig</a></span>, <span><a href=/author/roland-roller/>Roland Roller</a></span></div><span class=article-date>August 2023</span>
<span class=middot-divider></span><span class=pub-publication>KONVENS 2023</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/konvens2023-binsumait-etal-factuality/>Factuality Detection using Machine Translation - a Use Case for German Clinical Text</a></h3><a href=/publication/konvens2023-binsumait-etal-factuality/ class=summary-link><div class=article-style><p>Factuality can play an important role when automatically processing clinical text, as it makes a difference if particular symptoms are explicitly not present, possibly present, not mentioned, or affirmed. In most cases, a sufficient number of examples is necessary to handle such phenomena in a supervised machine learning setting. However, as clinical text might contain sensitive information, data cannot be easily shared. In the context of factuality detection, this work presents a simple solution using machine translation to translate English data to German to train a transformer-based factuality detection model.</p></div></a><div class=btn-links><button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/konvens2023-binsumait-etal-factuality/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/tatiana-anikina/>Tatiana Anikina</a></span></div><span class=article-date>June 2023</span>
<span class=middot-divider></span><span class=pub-publication>ACL SRW 2023</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/acl2023-anikina-dialogue/>Towards Efficient Dialogue Processing in the Emergency Response Domain</a></h3><a href=/publication/acl2023-anikina-dialogue/ class=summary-link><div class=article-style><p>In this paper we describe the task of adapting NLP models to dialogue processing in the emergency response domain. Our goal is to provide a recipe for building a system that performs dialogue act classification and domain-specific slot tagging while being efficient, flexible and robust. We show that adapter models (Pfeiffer et al., 2020) perform well in the emergency response domain and benefit from additional dialogue context and speaker information. Comparing adapters to standard fine-tuned Transformer models we show that they achieve competitive results and can easily accommodate new tasks without significant memory increase since the base model can be shared between the adapters specializing on different tasks. We also address the problem of scarce annotations in the emergency response domain and evaluate different data augmentation techniques in a low-resource setting.</p></div></a><div class=btn-links><button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/acl2023-anikina-dialogue/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/leonhard-hennig/>Leonhard Hennig</a></span>, <span><a href=/author/philippe-thomas/>Philippe Thomas</a></span>, <span><a href=/author/sebastian-moller/>Sebastian Möller</a></span></div><span class=article-date>May 2023</span>
<span class=middot-divider></span><span class=pub-publication>ACL 2023</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/acl2023-hennig-multitacred/>MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset</a></h3><a href=/publication/acl2023-hennig-multitacred/ class=summary-link><div class=article-style><p>Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2305.04582.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/acl2023-hennig-multitacred/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/MultiTACRED target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/vageesh-saxena/>Vageesh Saxena</a></span>, <span><a href=/author/nils-rethmeier/>Nils Rethmeier</a></span>, <span><a href=/author/gijs-van-dijck/>Gijs Van Dijck</a></span>, <span><a href=/author/gerasimos-spanakis/>Gerasimos Spanakis</a></span></div><span class=article-date>May 2023</span>
<span class=middot-divider></span><span class=pub-publication>ACL 2023</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/acl2023-rethmeier-vendorlink/>VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets</a></h3><a href=/publication/acl2023-rethmeier-vendorlink/ class=summary-link><div class=article-style><p>The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pretraining to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach can help Law Enforcement Agencies (LEA) make more informed decisions by verifying and identifying migrating vendors and their potential aliases on existing and Low-Resource (LR) emerging Darknet markets.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2305.02763v1.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/acl2023-rethmeier-vendorlink/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/maastrichtlawtech/VendorLink target=_blank rel=noopener>Code</a></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=sponsored class="home-section wg-blank"><div class=home-section-bg style=background-color:#fff></div><div class=container><div class="row justify-content-center"><div class=col-12><figure><a data-fancybox href=/media/bmbf_sponsored_and_logo_horizontal.png><img src=/media/bmbf_sponsored_and_logo_horizontal.png alt="Sponsored by the Federal Ministry of Education and Research"></a></figure></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Data Protection Notice</a>
&#183;
<a href=/terms/>Legal Information</a></p><p class=powered-by>© Cora4NLP 2020-2023</p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=false;</script><script src=/js/wowchemy.min.1521bbf8c12bfed32e2e1938f0a7fc26.js></script></body></html>