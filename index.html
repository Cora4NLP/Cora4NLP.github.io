<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Christoph Alt"><meta name=description content="Cora4NLP project page."><link rel=alternate hreflang=en-us href=https://cora4nlp.github.io/><meta name=theme-color content="#3f51b5"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.min.aee9166cce4168356878710aa1039ff5.css><link rel=alternate href=/index.xml type=application/rss+xml title=Cora4NLP><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://cora4nlp.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Cora4NLP"><meta property="og:url" content="https://cora4nlp.github.io/"><meta property="og:title" content="Cora4NLP"><meta property="og:description" content="Cora4NLP project page."><meta property="og:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2021-09-07T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","url":"https://cora4nlp.github.io"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cora4nlp.github.io","name":"Cora4NLP","logo":"https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png","address":{"@type":"PostalAddress","streetAddress":"Alt-Moabit 91c","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10559","addressCountry":"DE"},"url":"https://cora4nlp.github.io"}</script><title>Cora4NLP</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Cora4NLP</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Cora4NLP</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Contextual Reasoning and Adaptation for Natural Language Processing</h1></div><div class="col-12 col-lg-8"><p>Language is implicit — it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains — and transfer to, and between, both. Current language understanding methods, however, focus on high resource languages and domains, use little to no context, and assume static data, task, and target distributions.</p><p>The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project <a href=https://www.deeplee.de target=_blank rel=noopener>DEEPLEE</a> and is carried out jointly between DFKI&rsquo;s language technology research departments in <a href=https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/ target=_blank rel=noopener>Berlin</a> and <a href=https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/ target=_blank rel=noopener>Saarbrücken</a>. Specifically, our goal is to develop natural language understanding methods that enable:</p><ul><li>reasoning over broader co- and contexts</li><li>efficient adaptation to novel and/or low resource contexts</li><li>continual adaptation to, and generalization over, evolving contexts</li></ul><p>Cora4NLP is funded by the German Federal Ministry of Education and Research (BMBF) under funding code 01IW20010.</p></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Latest News</h1></div><div class="col-12 col-lg-8"></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/leonhard-hennig-and-phuc-tran-truong-and-aleksandra-gabryszak/>Leonhard Hennig and Phuc Tran Truong and Aleksandra Gabryszak</a></span></div><span class=article-date>September 2021</span>
<span class=middot-divider></span><span class=pub-publication>KONVENS 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/hennig-konvens-2021-mobie/>MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain</a></h3><a href=/publication/hennig-konvens-2021-mobie/ class=summary-link><div class=article-style><p>We present MobIE, a German-language dataset, which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,541 social media texts and traffic reports with 104K tokens, and contains 23,4K annotated entities, 15k of which are linked to a knowledge base. A subset of the dataset is human-annotated with seven mobility-related, n-ary relation types, while the remaining documents are annotated using a weakly-supervised labeling approach implemented with the Snorkel framework. To the best of our knowledge, this is the first German-language dataset that combines annotations for NER, EL and RE, and thus can be used for joint and multi-task learning of these fundamental information extraction tasks. We make MobIE public at <a href=https://github.com/dfki-nlp/mobie>https://github.com/dfki-nlp/mobie</a>.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/MobIE/blob/master/Konvens_2021_Daystream_Corpus.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/hennig-konvens-2021-mobie/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/MobIE/ target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/MobIE/ target=_blank rel=noopener>Dataset</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/jingyi-zhang/>Jingyi Zhang</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span></div><span class=article-date>August 2021</span>
<span class=middot-divider></span><span class=pub-publication>ACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/zhang-acl-2021-btba/>A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment</a></h3><a href=/publication/zhang-acl-2021-btba/ class=summary-link><div class=article-style><p>Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models. This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task. Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment). We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training. We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2021.acl-long.24.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/zhang-acl-2021-btba/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/hongfei-xu/>Hongfei Xu</a></span>, <span><a href=/author/qiuhui-liu/>Qiuhui Liu</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span>, <span><a href=/author/deyi-xiong/>Deyi Xiong</a></span></div><span class=article-date>August 2021</span>
<span class=middot-divider></span><span class=pub-publication>ACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/xu-acl-2021-multi/>Modeling Task-Aware {MIMO} Cardinality for Efficient Multilingual Neural Machine Translation</a></h3><a href=/publication/xu-acl-2021-multi/ class=summary-link><div class=article-style><p>Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2021.acl-short.46.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/xu-acl-2021-multi/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/hongfei-xu/>Hongfei Xu</a></span>, <span><a href=/author/qiuhui-liu/>Qiuhui Liu</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span>, <span><a href=/author/deyi-xiong/>Deyi Xiong</a></span>, <span><a href=/author/meng-zhang/>Meng Zhang</a></span></div><span class=article-date>August 2021</span>
<span class=middot-divider></span><span class=pub-publication>ACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/xu-acl-2021-lstm/>Multi-Head Highly Parallelized {LSTM} Decoder for Neural Machine Translation</a></h3><a href=/publication/xu-acl-2021-lstm/ class=summary-link><div class=article-style><p>One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is $O(n^2)$, increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state. This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this. To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context. This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations. We then connect the outputs of each parallel step with computationally cheap element-wise computations. We call this the Highly Parallelized LSTM. To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer. The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2021.acl-long.23.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/xu-acl-2021-lstm/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/hongfei-xu/>Hongfei Xu</a></span>, <span><a href=/author/josef-van-genabith/>Josef van Genabith</a></span>, <span><a href=/author/qiuhui-liu/>Qiuhui Liu</a></span>, <span><a href=/author/deyi-xiong/>Deyi Xiong</a></span></div><span class=article-date>April 2021</span>
<span class=middot-divider></span><span class=pub-publication>NAACL 2021</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/xu-naacl-2021-prob/>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</a></h3><a href=/publication/xu-naacl-2021-prob/ class=summary-link><div class=article-style><p>Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2021.naacl-main.7.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/xu-naacl-2021-prob/cite.bib>
Cite</button></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=sponsored class="home-section wg-blank"><div class=home-section-bg style=background-color:#fff></div><div class=container><div class="row justify-content-center"><div class=col-12><figure><a data-fancybox href=/media/bmbf_sponsored_and_logo_horizontal.png><img src=/media/bmbf_sponsored_and_logo_horizontal.png alt="Sponsored by the Federal Ministry of Education and Research"></a></figure></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© Cora4NLP 2021</p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=false;</script><script src=/js/wowchemy.min.1521bbf8c12bfed32e2e1938f0a7fc26.js></script></body></html>