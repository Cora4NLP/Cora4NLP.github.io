<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=author content="Christoph Alt"><meta name=description content="Cora4NLP project page."><link rel=alternate hreflang=en-us href=https://cora4nlp.github.io/><meta name=theme-color content="#3f51b5"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.min.aee9166cce4168356878710aa1039ff5.css><link rel=alternate href=/index.xml type=application/rss+xml title=Cora4NLP><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://cora4nlp.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Cora4NLP"><meta property="og:url" content="https://cora4nlp.github.io/"><meta property="og:title" content="Cora4NLP"><meta property="og:description" content="Cora4NLP project page."><meta property="og:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-05-26T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","url":"https://cora4nlp.github.io"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cora4nlp.github.io","name":"Cora4NLP","logo":"https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png","address":{"@type":"PostalAddress","streetAddress":"Alt-Moabit 91c","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10559","addressCountry":"DE"},"url":"https://cora4nlp.github.io"}</script><title>Cora4NLP</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Cora4NLP</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Cora4NLP</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Contextual Reasoning and Adaptation for Natural Language Processing</h1></div><div class="col-12 col-lg-8"><p>Language is implicit — it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains — and transfer to, and between, both. Current language understanding methods, however, focus on high resource languages and domains, use little to no context, and assume static data, task, and target distributions.</p><p>The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project <a href=https://www.deeplee.de target=_blank rel=noopener>DEEPLEE</a> and is carried out jointly between DFKI&rsquo;s language technology research departments in <a href=https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/ target=_blank rel=noopener>Berlin</a> and <a href=https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/ target=_blank rel=noopener>Saarbrücken</a>. Specifically, our goal is to develop natural language understanding methods that enable:</p><ul><li>reasoning over broader co- and contexts</li><li>efficient adaptation to novel and/or low resource contexts</li><li>continual adaptation to, and generalization over, evolving contexts</li></ul><p>Cora4NLP is funded by the German Federal Ministry of Education and Research (BMBF) under funding code 01IW20010.</p></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Latest News</h1></div><div class="col-12 col-lg-8"></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/yuxuan-chen-and-jonas-mikkelsen-and-christoph-alt-and-leonhard-hennig/>Yuxuan Chen and Jonas Mikkelsen and Christoph Alt and Leonhard Hennig</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-REPL4NLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/chen-acl-repl4nlp-2022-fewie/></a></h3><a href=/publication/chen-acl-repl4nlp-2022-fewie/ class=summary-link><div class=article-style><p>Pre-trained language models (PLM) are effective components of few-shot named entity recognition (NER) approaches when augmented with continued pre-training on task-specific out-of-domain data or fine-tuning on in-domain data. However, their performance in low-resource scenarios, where such data is not available, remains an open question. We introduce an encoder evaluation framework, and use it to systematically compare the performance of state-of-the-art pre-trained representations on the task of low-resource NER. We analyze a wide range of encoders pre-trained with different strategies, model architectures, intermediate-task fine-tuning, and contrastive learning. Our experimental results across ten benchmark NER datasets in English and German show that encoder performance varies significantly, suggesting that the choice of encoder for a specific low-resource scenario needs to be carefully evaluated.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/fewie/blob/master/REPL4NLP_2022_Fewie.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/chen-acl-repl4nlp-2022-fewie/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/fewie/ target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/noon-pokaratsiri-goldstein/>Noon Pokaratsiri Goldstein</a></span>, <span><a href=/author/morgan-k.-wixted/>Morgan K. Wixted</a></span>, <span><a href=/author/alejandro-p.-garcia-rudolph/>Alejandro P. Garcia-Rudolph</a></span>, <span><a href=/author/catalina-martinez-costa/>Catalina Martínez-Costa</a></span>, <span><a href=/author/gunter-neumann/>Günter Neumann</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-BioNLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/amin-acl-bionlp22-fewdeid/>Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts</a></h3><a href=/publication/amin-acl-bionlp22-fewdeid/ class=summary-link><div class=article-style><p>Despite the advances in digital healthcare systems offering curated structured knowledge, much of the important information still lies in large-volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected health information (PHI), are subject to information extraction tools for downstream applications, risking the identification of patients. Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not reproducible in real-world multilingual settings. Pre-trained language models (LM) have shown great potential of transfer learning in low-resource settings. In this work, we showcase the adaptation of LMs in few-shot cross-lingual transfer in solving a real-world challenge—Spanish-Catalan code-mixed clinical notes de-identification in the stroke domain. We annotate a gold evaluation dataset to assess few-shot setting performance where only a few hundred labeled examples are used for training. Our model boosts the zero-shot F1-score from 73.7% to 91.2% in a multi-task setup using the MEDDOCAN corpus (Marimon et al., 2019) and our code-mixed domain-constrained few-shot corpus with Multilingual BERT (mBERT) (Devlin et al., 2019). Our best model achieves a human-evaluation F1-score of 97.2% when generalized to an out-of-sample test set.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://drive.google.com/file/d/1ObHumDgKSfFs_Ufj271rFIoJMSYwnpGz/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/amin-acl-bionlp22-fewdeid/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/pasquale-minervini/>Pasquale Minervini</a></span>, <span><a href=/author/david-chang/>David Chang</a></span>, <span><a href=/author/gunter-neumann/>Günter Neumann</a></span>, <span><a href=/author/pontus-stenetorp/>Pontus Stenetorp</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-BioNLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/amin-acl-bionlp22-md19/>MedDistant19: A Challenging Benchmark for Distantly Supervised Biomedical Relation Extraction</a></h3><a href=/publication/amin-acl-bionlp22-md19/ class=summary-link><div class=article-style><p>Relation Extraction in the biomedical domain is a challenging task due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used as a way to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. In several benchmarks, Distantly Supervised Biomedical Relation Extraction (Bio-DSRE) models can seemingly produce very accurate results. However, given the challenging nature of the task, we set out to investigate the validity of such impressive results. We probed the datasets used by Amin et al. (2020) and Hogan et al. (2021) and found a significant overlap between training and evaluation relationships that, once resolved, reduced the accuracy of the models by up to 71%. Furthermore, we noticed several inconsistencies with the data construction process, such as the creation of negative samples and improper handling of redundant relationships. We mitigate these issues and present MEDDISTANT19, a new benchmark dataset obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms (SNOMED CT) knowledge base. We experimented with several state-of-the-art models achieving an AUC of 55.4% and 49.8% at sentence- and bag-level, showing that there is still plenty of room for improvement. We will release our code and data for reproducibility.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://drive.google.com/file/d/1SoEOEndaAYX3kJxpb7Y_SfQUSGCZo6hp/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/amin-acl-bionlp22-md19/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/suamin/MedDistant19 target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/ioannis-dikeoulias/>Ioannis Dikeoulias</a></span>, <span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/gunter-neumann/>Günter Neumann</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-RepL4NLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/dikeoulias-acl-repl4nlp-tlowfer/>Temporal Knowledge Graph Reasoning with Low-rank and Model-agnostic Representations</a></h3><a href=/publication/dikeoulias-acl-repl4nlp-tlowfer/ class=summary-link><div class=article-style><p>Temporal knowledge graph completion (TKGC) is a predominant task for reasoning over event and temporal knowledge graphs, targeting the completion of knowledge with true, but missing information. In this context, tensor decomposition has shown great success to model interactions between entities and relations. Their effectiveness in static knowledge graph completion motivates us to introduce Time-LowFER, a family of parameter-efficient and time-aware extensions of the low-rank tensor factorization model LowFER. Noting several limitations in current approaches to represent time, we also propose a cycle-aware time-encoding scheme for time features, which is model-agnostic and offers a more generalized representation of time. We implement our methods in a unified temporal knowledge graph embedding framework, with a focus on time-sensitive data processing. In the experiments, we show that our proposed methods perform on par or better than the state-of-the-art semantic matching models on three benchmark event knowledge graphs.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://drive.google.com/file/d/1nz-gZr0YuCYIy-NJ-biyAbHMiAiVMIF9/view?usp=sharing" target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/dikeoulias-acl-repl4nlp-tlowfer/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/iodike/ChronoKGE target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/n.-rethmeier/>N. Rethmeier</a></span>, <span><a href=/author/i.-augenstein/>I. Augenstein</a></span></div><span class=article-date>February 2022</span>
<span class=middot-divider></span><span class=pub-publication>AIBSD</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/rethmeier-aaai-2022-longtail/>Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data</a></h3><a href=/publication/rethmeier-aaai-2022-longtail/ class=summary-link><div class=article-style><p>For natural language processing &lsquo;text-to-text&rsquo; tasks, the prevailing approaches heavily rely on pretraining large self-supervised models on increasingly larger &lsquo;task-external&rsquo; data. Transfer learning from high-resource pretraining works well, but research has focused on settings with very large data and compute requirements, while the potential of efficient low-resource learning, without large &lsquo;task-external&rsquo; pretraining, remains under-explored. In this work, we evaluate against three core challenges for resource efficient learning. Namely, we analyze: (1) pretraining data (X) efficiency; (2) zero to few-shot label (Y) efficiency; and (3) long-tail generalization, since long-tail preservation has been linked to algorithmic fairness and because data in the tail is limited by definition. To address these challenges, we propose a data and compute efficient self-supervised, contrastive text encoder, pretrained on 60MB of &lsquo;task-internal&rsquo; text data, and compare it to RoBERTa, which was pretrained on 160GB of &lsquo;task-external&rsquo; text. We find our method outperforms RoBERTa, while pretraining and fine-tuning in a 1/5th of RoBERTa&rsquo;s fine-tuning time.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://arxiv.org/pdf/2010.01061 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/rethmeier-aaai-2022-longtail/cite.bib>
Cite</button></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=sponsored class="home-section wg-blank"><div class=home-section-bg style=background-color:#fff></div><div class=container><div class="row justify-content-center"><div class=col-12><figure><a data-fancybox href=/media/bmbf_sponsored_and_logo_horizontal.png><img src=/media/bmbf_sponsored_and_logo_horizontal.png alt="Sponsored by the Federal Ministry of Education and Research"></a></figure></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>© Cora4NLP 2020-2022</p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=false;</script><script src=/js/wowchemy.min.1521bbf8c12bfed32e2e1938f0a7fc26.js></script></body></html>