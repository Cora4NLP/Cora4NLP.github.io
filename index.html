<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.0.0-beta.1 for Hugo"><meta name=description content="Cora4NLP project page."><link rel=alternate hreflang=en-us href=https://cora4nlp.github.io/><meta name=theme-color content="#3f51b5"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin=anonymous async></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.min.aee9166cce4168356878710aa1039ff5.css><link rel=alternate href=/index.xml type=application/rss+xml title=Cora4NLP><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://cora4nlp.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Cora4NLP"><meta property="og:url" content="https://cora4nlp.github.io/"><meta property="og:title" content="Cora4NLP"><meta property="og:description" content="Cora4NLP project page."><meta property="og:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-10-21T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","url":"https://cora4nlp.github.io"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","@id":"https://cora4nlp.github.io","name":"Cora4NLP","logo":"https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png","address":{"@type":"PostalAddress","streetAddress":"Alt-Moabit 91c","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10559","addressCountry":"DE"},"url":"https://cora4nlp.github.io"}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#3f51b5","text":"rgb(255, 255, 255)"},"button":{"background":"rgb(255, 255, 255)","text":"#3f51b5"}},"theme":"classic","content":{"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"Got it!","link":"Learn more","href":"/privacy/"}})});</script><title>Cora4NLP</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Cora4NLP</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Cora4NLP</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#news data-target=#news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><span class="js-widget-page d-none"></span><section id=welcome class="home-section wg-blank"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Contextual Reasoning and Adaptation for Natural Language Processing</h1></div><div class="col-12 col-lg-8"><p>Language is implicit — it omits information. Filling this information gap requires contextual inference, background- and commonsense knowledge, and reasoning over situational context. Language also evolves, i.e., it specializes and changes over time. For example, many different languages and domains exist, new domains arise, and both evolve constantly. Thus, language understanding also requires continuous and efficient adaptation to new languages and domains — and transfer to, and between, both. Current language understanding methods, however, focus on high resource languages and domains, use little to no context, and assume static data, task, and target distributions.</p><p>The research in Cora4NLP aims to address these challenges. It builds on the expertise and results of the predecessor project <a href=https://www.deeplee.de target=_blank rel=noopener>DEEPLEE</a> and is carried out jointly between DFKI&rsquo;s language technology research departments in <a href=https://www.dfki.de/en/web/research/research-departments/speech-and-language-technology/ target=_blank rel=noopener>Berlin</a> and <a href=https://www.dfki.de/en/web/research/research-departments/multilinguality-and-language-technology/ target=_blank rel=noopener>Saarbrücken</a>. Specifically, our goal is to develop natural language understanding methods that enable:</p><ul><li>reasoning over broader co- and contexts</li><li>efficient adaptation to novel and/or low resource contexts</li><li>continual adaptation to, and generalization over, evolving contexts</li></ul><p>Cora4NLP is funded by the German Federal Ministry of Education and Research (BMBF) under funding code 01IW20010.</p></div></div></div></section><section id=news class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Latest News</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><span class=article-date>Oct 14, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/emnlp2022/>1 paper to be presented at EMNLP 2022</a></h3><a href=/post/emnlp2022/ class=summary-link><div class=article-style><p>One paper from Cora4NLP authors has been accepted for publication at EMNLP 2022, the 2022 Conference on Empirical Methods in Natural Language Processing. The conference is planned to be a hybrid meeting and will take place in Abu Dhabi, from Dec 7th to Dec 11th, 2022.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Oct 14, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/aacl-ijcnlp2022/>1 paper to be presented at AACL-IJCNLP 2022</a></h3><a href=/post/aacl-ijcnlp2022/ class=summary-link><div class=article-style><p>One paper from DFKI-NLP authors has been accepted for publication at the Workshop on Information Extraction from Scientific Publications (WIESP). The workshop will be held at AACL-IJCNLP 2022, the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, which will take place as an online-only event from Nov.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Mar 31, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>1 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/coling2022/>1 paper to be presented at COLING 2022</a></h3><a href=/post/coling2022/ class=summary-link><div class=article-style><p>One paper from Cora4NLP researchers has been accepted for oral presentation at COLING 2022, the 29th International Conference on Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Gyeongju, Korea, from Oct 12th - Oct 17th, 2022.</p></div></a></div><div class=card-simple><div class=article-metadata><span class=article-date>Mar 31, 2022</span>
<span class=middot-divider></span><span class=article-reading-time>2 min read</span></div><h3 class="article-title mb-1 mt-3"><a href=/post/acl2022/>4 papers to be presented at ACL 2022</a></h3><a href=/post/acl2022/ class=summary-link><div class=article-style><p>Four papers from Cora4NLP researchers have been accepted for publication at ACL 2022, the 60th Annual Meeting of the Association for Computational Linguistics. The conference is planned to be a hybrid meeting and will take place in Dublin, Ireland, from May 22nd through May 27th, 2022.</p></div></a></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4 section-heading"><h1>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class=card-simple><div class=article-metadata><div><span><a href=/author/arne-binder/>Arne Binder</a></span>, <span><a href=/author/bhuvanesh-verma/>Bhuvanesh Verma</a></span>, <span><a href=/author/leonhard-hennig/>Leonhard Hennig</a></span></div><span class=article-date>October 2022</span>
<span class=middot-divider></span><span class=pub-publication>WIESP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/wiesp2022-binder-etal-full/>Full-Text Argumentation Mining on Scientific Publications</a></h3><a href=/publication/wiesp2022-binder-etal-full/ class=summary-link><div class=article-style><p>Scholarly Argumentation Mining (SAM) has recently gained attention due to its potential to help scholars with the rapid growth of published scientific literature. It comprises two subtasks: argumentative discourse unit recognition (ADUR) and argumentative relation extraction (ARE), both of which are challenging since they require e.g. the integration of domain knowledge, the detection of implicit statements, and the disambiguation of argument structure. While previous work focused on dataset construction and baseline methods for specific document sections, such as abstract or results, full-text scholarly argumentation mining has seen little progress. In this work, we introduce a sequential pipeline model combining ADUR and ARE for full-text SAM, and provide a first analysis of the performance of pretrained language models (PLMs) on both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus, outperforming the previous best reported result by a large margin (+7% F1). We also present the first results for ARE, and thus for the full AM pipeline, on this benchmark dataset. Our detailed error analysis reveals that non-contiguous ADUs as well as the interpretation of discourse connectors pose major challenges and that data annotation needs to be more consistent.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://raw.githubusercontent.com/DFKI-NLP/sam/main/Argumentation_Mining_on_Sci-Arg.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/wiesp2022-binder-etal-full/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/sam target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/pasquale-minervini/>Pasquale Minervini</a></span>, <span><a href=/author/david-chang/>David Chang</a></span>, <span><a href=/author/pontus-stenetorp/>Pontus Stenetorp</a></span>, <span><a href=/author/gunter-neumann/>Günter Neumann</a></span></div><span class=article-date>October 2022</span>
<span class=middot-divider></span><span class=pub-publication>COLING 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/amin-coling2022-md19/>MedDistant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction</a></h3><a href=/publication/amin-coling2022-md19/ class=summary-link><div class=article-style><p>Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26% to 86%. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2022.coling-1.198.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/amin-coling2022-md19/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/pminervini/meddistant-baselines target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/suamin/MedDistant19 target=_blank rel=noopener>Dataset</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/yuxuan-chen/>Yuxuan Chen</a></span>, <span><a href=/author/jonas-mikkelsen/>Jonas Mikkelsen</a></span>, <span><a href=/author/arne-binder/>Arne Binder</a></span>, <span><a href=/author/christoph-alt/>Christoph Alt</a></span>, <span><a href=/author/leonhard-hennig/>Leonhard Hennig</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-REPL4NLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/acl2022-repl4nlp-chen-fewie/>A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition</a></h3><a href=/publication/acl2022-repl4nlp-chen-fewie/ class=summary-link><div class=article-style><p>Pre-trained language models (PLM) are effective components of few-shot named entity recognition (NER) approaches when augmented with continued pre-training on task-specific out-of-domain data or fine-tuning on in-domain data. However, their performance in low-resource scenarios, where such data is not available, remains an open question. We introduce an encoder evaluation framework, and use it to systematically compare the performance of state-of-the-art pre-trained representations on the task of low-resource NER. We analyze a wide range of encoders pre-trained with different strategies, model architectures, intermediate-task fine-tuning, and contrastive learning. Our experimental results across ten benchmark NER datasets in English and German show that encoder performance varies significantly, suggesting that the choice of encoder for a specific low-resource scenario needs to be carefully evaluated.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/fewie/blob/master/REPL4NLP_2022_Fewie.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/acl2022-repl4nlp-chen-fewie/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://github.com/DFKI-NLP/fewie/ target=_blank rel=noopener>Code</a></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/david-harbecke/>David Harbecke</a></span>, <span><a href=/author/yuxuan-chen/>Yuxuan Chen</a></span>, <span><a href=/author/leonhard-hennig/>Leonhard Hennig</a></span>, <span><a href=/author/christoph-alt/>Christoph Alt</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-NLPPower 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/acl2022-nlppower-harbecke-f1/>Why only Micro-$F_1$? Class Weighting of Measures for Relation Classification</a></h3><a href=/publication/acl2022-nlppower-harbecke-f1/ class=summary-link><div class=article-style><p>Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F1, macro-F1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.</p></div></a><div class=btn-links><button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/acl2022-nlppower-harbecke-f1/cite.bib>
Cite</button></div></div><div class=card-simple><div class=article-metadata><div><span><a href=/author/saadullah-amin/>Saadullah Amin</a></span>, <span><a href=/author/noon-pokaratsiri-goldstein/>Noon Pokaratsiri Goldstein</a></span>, <span><a href=/author/morgan-wixted/>Morgan Wixted</a></span>, <span><a href=/author/alejandro-garcia-rudolph/>Alejandro Garcia-Rudolph</a></span>, <span><a href=/author/catalina-martinez-costa/>Catalina Martinez-Costa</a></span>, <span><a href=/author/and-guenter-neumann/>and Guenter Neumann</a></span></div><span class=article-date>May 2022</span>
<span class=middot-divider></span><span class=pub-publication>ACL-BioNLP 2022</span></div><h3 class="article-title mb-1 mt-3"><a href=/publication/amin-acl-bionlp22-fewdeid/>Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts</a></h3><a href=/publication/amin-acl-bionlp22-fewdeid/ class=summary-link><div class=article-style><p>Despite the advances in digital healthcare systems offering curated structured knowledge, much of the critical information still lies in large volumes of unlabeled and unstructured clinical texts. These texts, which often contain protected health information (PHI), are exposed to information extraction tools for downstream applications, risking patient identification. Existing works in de-identification rely on using large-scale annotated corpora in English, which often are not suitable in real-world multilingual settings. Pre-trained language models (LM) have shown great potential for cross-lingual transfer in low-resource settings. In this work, we empirically show the few-shot cross-lingual transfer property of LMs for named entity recognition (NER) and apply it to solve a low-resource and real-world challenge of code-mixed (Spanish-Catalan) clinical notes de-identification in the stroke domain. We annotate a gold evaluation dataset to assess few-shot setting performance where we only use a few hundred labeled examples for training. Our model improves the zero-shot F1-score from 73.7% to 91.2% on the gold evaluation set when adapting Multilingual BERT (mBERT) (Devlin et al., 2019) from the MEDDOCAN (Marimon et al., 2019) corpus with our few-shot cross-lingual target corpus. When generalized to an out-of-sample test set, the best model achieves a human-evaluation F1-score of 97.2%.</p></div></a><div class=btn-links><a class="btn btn-outline-primary my-1 mr-1 btn-sm" href=https://aclanthology.org/2022.bionlp-1.20.pdf target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal" data-filename=/publication/amin-acl-bionlp22-fewdeid/cite.bib>
Cite</button></div></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=sponsored class="home-section wg-blank"><div class=home-section-bg style=background-color:#fff></div><div class=container><div class="row justify-content-center"><div class=col-12><figure><a data-fancybox href=/media/bmbf_sponsored_and_logo_horizontal.png><img src=/media/bmbf_sponsored_and_logo_horizontal.png alt="Sponsored by the Federal Ministry of Education and Research"></a></figure></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by><a href=/privacy/>Data Protection Notice</a>
&#183;
<a href=/terms/>Legal Information</a></p><p class=powered-by>© Cora4NLP 2020-2022</p><p class=powered-by>Published with
<a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> —
the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin=anonymous></script><script>const code_highlighting=false;</script><script src=/js/wowchemy.min.1521bbf8c12bfed32e2e1938f0a7fc26.js></script></body></html>