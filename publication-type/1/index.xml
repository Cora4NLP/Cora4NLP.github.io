<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>1 | Cora4NLP</title><link>https://cora4nlp.github.io/publication-type/1/</link><atom:link href="https://cora4nlp.github.io/publication-type/1/index.xml" rel="self" type="application/rss+xml"/><description>1</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Cora4NLP 2021</copyright><lastBuildDate>Tue, 07 Sep 2021 00:00:00 +0000</lastBuildDate><image><url>https://cora4nlp.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url><title>1</title><link>https://cora4nlp.github.io/publication-type/1/</link></image><item><title>MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain</title><link>https://cora4nlp.github.io/publication/hennig-konvens-2021-mobie/</link><pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/hennig-konvens-2021-mobie/</guid><description/></item><item><title>Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages</title><link>https://cora4nlp.github.io/publication/ruiter-mtsummit-2021/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/ruiter-mtsummit-2021/</guid><description/></item><item><title>A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment</title><link>https://cora4nlp.github.io/publication/zhang-acl-2021-btba/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/zhang-acl-2021-btba/</guid><description/></item><item><title>Modeling Task-Aware {MIMO} Cardinality for Efficient Multilingual Neural Machine Translation</title><link>https://cora4nlp.github.io/publication/xu-acl-2021-multi/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/xu-acl-2021-multi/</guid><description/></item><item><title>Multi-Head Highly Parallelized {LSTM} Decoder for Neural Machine Translation</title><link>https://cora4nlp.github.io/publication/xu-acl-2021-lstm/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/xu-acl-2021-lstm/</guid><description/></item><item><title>Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers</title><link>https://cora4nlp.github.io/publication/xu-naacl-2021-prob/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/xu-naacl-2021-prob/</guid><description/></item><item><title>T2NER: Transformers Based Transfer Learning Framework for Named Entity Recognition</title><link>https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/</link><pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/amin-eacl-2021-t2ner/</guid><description/></item><item><title>A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned and Perspectives</title><link>https://cora4nlp.github.io/publication/rethmeier-arxiv-2021-primer/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/rethmeier-arxiv-2021-primer/</guid><description/></item><item><title>MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation</title><link>https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/</link><pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/adelani-eacl-2021-menyo20k/</guid><description/></item><item><title>Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data</title><link>https://cora4nlp.github.io/publication/rethmeier-arxiv-2020-longtail/</link><pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate><guid>https://cora4nlp.github.io/publication/rethmeier-arxiv-2020-longtail/</guid><description/></item></channel></rss>